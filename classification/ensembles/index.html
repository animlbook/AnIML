

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Ensemble Methods &#8212; AnIML: Another Introduction to Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "animlbook/AnIML");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "preferred-color-scheme");
    script.setAttribute("label", "💬 Comments");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script src="../../_static/script.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmin": ["\\underset{#1}{\\operatorname{argmin}}", 1], "argmax": ["\\underset{#1}{\\operatorname{argmax}}", 1], "abs": ["\\lvert #1 \\rvert", 1], "indicator": ["\\mathbb{\\unicode{x1D7D9}}\\left\\{ #1 \\right\\}", 1], "norm": ["\\lVert #1 \\rVert", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'classification/ensembles/index';</script>
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="11. Neural Networks" href="../../deep_learning/intro/index.html" />
    <link rel="prev" title="9. Decision Trees" href="../trees/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">This book is still under construction. We appreciate your patience as we get it completed. Feedback is welcome!</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro/index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro/index.html">
                    <i class="fas fa-hand-sparkles fa-fw"></i> Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../regression/linear_regression/index.html">1. <i class="fas fa-book fa-fw"></i> Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/assessing_performance/index.html">2. <i class="fas fa-book fa-fw"></i> Assessing Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/ridge/index.html">3. <i class="fas fa-book fa-fw"></i> Ridge Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/lasso/index.html">4. <i class="fas fa-book fa-fw"></i> Feature Selection and LASSO Regularization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/index.html">5. <i class="fas fa-book fa-fw"></i> Classification Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logistic_regression/index.html">6. <i class="fas fa-book fa-fw"></i> Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bias_fairness/index.html">7. <i class="fas fa-book fa-fw"></i> Bias and Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../naive_bayes/index.html">8. <i class="fas fa-book fa-fw"></i> Naïve Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trees/index.html">9. <i class="fas fa-book fa-fw"></i> Decision Trees</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. <i class="fas fa-book fa-fw"></i> Ensemble Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../deep_learning/intro/index.html">11. <i class="fas fa-book fa-fw"></i> Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_learning/conv_nets/index.html">12. <i class="fas fa-book fa-fw"></i> Convolutional Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Document Retrieval / Local Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/knn/index.html">13. <i class="fas fa-book fa-fw"></i> Introduction, Precision/Recall, k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/kernels/index.html">14. <i class="fas fa-book fa-fw"></i> Kernel Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/lsh/index.html">15. <i class="fas fa-book fa-fw"></i> Locality Sensitive Hashing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/clustering/index.html">16. <i class="fas fa-book fa-fw"></i> Clustering &amp; k-means</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/other_clustering/index.html">17. <i class="fas fa-book fa-fw"></i> Hierarchical Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommender Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../recommender_systems/pca/index.html">18. <i class="fas fa-book fa-fw"></i> Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recommender_systems/recommendation/index.html">19. <i class="fas fa-book fa-fw"></i> Recommender Systems</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/animlbook/AnIML/main?urlpath=lab/tree/book_source/source/classification/ensembles/index.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/animlbook/AnIML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/animlbook/AnIML/edit/main/book_source/source/classification/ensembles/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/animlbook/AnIML/issues/new?title=Issue%20on%20page%20%2Fclassification/ensembles/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/classification/ensembles/index.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/classification/ensembles/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1><i class="fas fa-book fa-fw"></i> Ensemble Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aside-data-types">10.1. Aside: Data Types</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-data">10.1.1. Numeric Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-data">10.1.2. Categorical Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking">10.2. Stacking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-random-forests">10.3. Bagging - Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-algorithm">10.3.1. Random Forest Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-properties">10.3.2. Random Forest Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-code">10.3.3. Random Forest Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practicalities">10.3.4. Practicalities</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-features-m">10.3.4.1. Number of features <span class="math notranslate nohighlight">\(m\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-oob-error">10.3.4.2. Out of Bag (OOB) Error</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting-adaboost">10.4. Boosting - AdaBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-predictions">10.4.1. AdaBoost Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-training">10.4.2. AdaBoost Training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-from-weighted-data">10.4.2.1. Learning from Weighted Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-weights-hat-w-t">10.4.2.2. Model Weights <span class="math notranslate nohighlight">\(\hat{w}_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-dataset-weights-alpha-i-t-1">10.4.2.3. Computing Dataset Weights $\alpha_{i, t+1}</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">10.4.2.4. Putting it all Together</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-code">10.4.3. AdaBoost code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-walkthrough">10.4.4. AdaBoost Walkthrough</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">10.4.5. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-properties">10.4.6. AdaBoost Properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-reflection">10.5. Recap / Reflection</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="i-class-fas-fa-book-fa-fw-i-ensemble-methods">
<h1><span class="section-number">10. </span><i class="fas fa-book fa-fw"></i> Ensemble Methods<a class="headerlink" href="#i-class-fas-fa-book-fa-fw-i-ensemble-methods" title="Permalink to this heading">#</a></h1>
<p>Last time, we discussed the details of training and evaluating our decision tree model. In this chapter, we will discuss the powerful concept of <strong>ensemble models</strong>, or models composed of a group of smaller models. The idea is that maybe in combination, models that individually don’t perform well individually, might perform better collectively. This follows an observation of “the wisdom of the crowd” where aggregate judgements often can outperform judgements of even an informed individually. One example of the wisdom-of-the-crowd comes from Francis Galton’s observation of game at a fair in 1906 where 787 participants tried to guess the weight of a cow. This group of people included experts in cows such as farmers and butchers as well as regular folk who all guessed its weight. Surprisingly, the best guess for the weight was actually the <em>average guess</em> across all 787 participants; the average guess was 1196 lbs while the true weight was 1198 lbs. This average guess was much closer than any individual’s guess, including the guesses of the experts.</p>
<p>Applying this idea to machine learning, we can hope to make a more accurate model by combining the guesses of individual models that might not be as effective on their own. There are many different approaches to how to combine models of various types into an ensemble, but they generally fall into three categories that we will explore in this chapter.</p>
<ul class="simple">
<li><p><strong>Stacking</strong></p></li>
<li><p><strong>Bagging</strong></p></li>
<li><p><strong>Boosting</strong></p></li>
</ul>
<p>Note that these are broad classes that describe many types of models. In this chapter, we will primarily only explore one type of model for each category.</p>
<section id="aside-data-types">
<h2><span class="section-number">10.1. </span>Aside: Data Types<a class="headerlink" href="#aside-data-types" title="Permalink to this heading">#</a></h2>
<p>Before diving into the various types of ensemble models, we want to take a step back to have a discussion about the different types of data we will train our models on. This discussion has nothing to do with ensemble models directly, but is a more general concept when thinking of our machine learning pipeline.</p>
<p>Data that we receive can be varied in their formats and types. Very generally, there are two extremely common types of data that we see especially in most of the tabular data we have been working with (each with their own subtypes).</p>
<ul class="simple">
<li><p><strong>Numeric Data</strong>  describes data representing numbers (quantitative)</p></li>
<li><p><strong>Categorical Data</strong> describes data representing distinct categories (qualitative)</p></li>
</ul>
<section id="numeric-data">
<h3><span class="section-number">10.1.1. </span>Numeric Data<a class="headerlink" href="#numeric-data" title="Permalink to this heading">#</a></h3>
<p>Data describing numbers generally comes in one of two types.</p>
<ul class="simple">
<li><p><strong>Discrete</strong> values that cannot be subdivided. For example, in our house price predicting example the features representing the number of bedrooms were integers (1, 2, 3, …) that cannot be subdivided further.</p></li>
<li><p><strong>Continuous</strong> values can be subdivided. For example, the area of a house is a real number that can be infinitesimally subdivided (assuming precise enough measurements).</p></li>
</ul>
<p>There is a tricky case of numeric values such as a price of the house. While it seems more like a continuous value, you can only break the units so far down to cents before not being able to go further. You generally have to consider if you would like to treat these variables as discrete or continuous. A general rule of thumb is if the discreteness comes from the unit of measurement (a bedroom has to be a whole number), then it should be discrete but if the discreteness comes from the quantity being measured (prices can only be broken down to cents), you can treat it as continuous. You will likely just need an extra step to round outputs to be the precision that is appropriate for the problem.</p>
<p>All of the ML models that we have described so far assume numeric inputs so very rarely is there preprocessing you need to do to make numeric data work. You may have to consider things like rounding as we described above based on the limitations of the numeric data precision, but generally that’s not something you have to worry too much about.</p>
</section>
<section id="categorical-data">
<h3><span class="section-number">10.1.2. </span>Categorical Data<a class="headerlink" href="#categorical-data" title="Permalink to this heading">#</a></h3>
<p>Data describing categories also generally comes in one of two types.</p>
<ul class="simple">
<li><p><strong>Ordinal</strong> data is categorical data that has a defined order. For example, a school rating of good/okay/bad has a clear ordering of which category can be considered greater than another.</p></li>
<li><p><strong>Nominal</strong> data is categorical data that does not have a defined order. For example, the type of school you attend being public/private/charter/homeschool does not have an ordering of which one of those school is greater than another (although everyone might have an opinion about which one may be best).</p></li>
</ul>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>1. While this is true in theory, <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> does not implement their tree like this so you actually do have to transform the values when using that library for all model types.</p>
</aside>
<p>As we mentioned before, all of the ML models we have discussed so far (with the exception of Decision Trees<sup>1</sup>) have assumed the inputs are numeric values. That means in order to train these models on data with categorical features, we have to do some preprocessing.</p>
<p>How might we go about transforming categorical variables into numeric ones? One natural idea is to use what we might call a <em>value encoding</em> to create a mapping from each category to a number. So for example of good/okay/bad, we could make the following mapping:</p>
<ul class="simple">
<li><p>Good = 1</p></li>
<li><p>Okay = 0</p></li>
<li><p>Bad = -1</p></li>
</ul>
<p>This actually works fine in practice with ordinal data, if we choose our mappings to respect the ordering of the categories. However, this setup doesn’t work at all with nominal values. Consider our example of a category for school type with the values public/private/charter/homeschool. We could come up with a value encoding such as the following.</p>
<ul class="simple">
<li><p>Public = 1</p></li>
<li><p>Private = 2</p></li>
<li><p>Charter = 3</p></li>
<li><p>Homeschool = 4</p></li>
</ul>
<p>By choosing this encoding for nominal values though has now introduced some problems in our data that we might not want. In particular:</p>
<ul class="simple">
<li><p>We have now defined an implicit ordering between the categories (Homeschool &gt; Public) even though as nominal values, they are not supposed to have such an ordering.</p></li>
<li><p>We have also added unintended relationships between how our model may consider the various feature values. Since they are just numbers to the model, the model would expect this feature to behave like any numeric feature would. That means you would expect mathematical statements such as Public (1) + Charter (3) = Homeschool (4) or Private (2) * Private (2) - Public (1) = Charter (3). These spurious numeric relationships are a byproduct of how we expect numbers to behave, so representing nominal values in this way will create completely unexpected relationships in our data.</p>
<ul>
<li><p>This technically is also a critique of using value encodings for ordinal data, but because many models might only care about the relative ordering of feature values it generally doesn’t cause problems.</p></li>
</ul>
</li>
</ul>
<p>To fix this, we will need a slightly more complicated encoding. One of the most common encoding types is a <strong>one-hot encoding</strong> that we briefly showed in a code example in the last chapter. A one-hot encoding turns a categorical feature into many categorical features, one for each value that feature took on, and indicates with a 1 if that example had that feature as a value. This is clearer with an example.</p>
<p>Consider a small school dataset with the following columns.</p>
<table class="table" id="id1">
<caption><span class="caption-number">Table 10.1 </span><span class="caption-text">Raw School Dataset</span><a class="headerlink" href="#id1" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>School</p></th>
<th class="head"><p>Sq. Ft.</p></th>
<th class="head"><p>Rating</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Public</p></td>
<td><p>1000</p></td>
<td><p>Good</p></td>
</tr>
<tr class="row-odd"><td><p>Private</p></td>
<td><p>1500</p></td>
<td><p>Bad</p></td>
</tr>
<tr class="row-even"><td><p>Charter</p></td>
<td><p>700</p></td>
<td><p>Good</p></td>
</tr>
<tr class="row-odd"><td><p>Private</p></td>
<td><p>1200</p></td>
<td><p>Good</p></td>
</tr>
</tbody>
</table>
<table class="table" id="id2">
<caption><span class="caption-number">Table 10.2 </span><span class="caption-text">One-Hot Encoded Dataset</span><a class="headerlink" href="#id2" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>School  - Public</p></th>
<th class="head"><p>School - Private</p></th>
<th class="head"><p>School - Charter</p></th>
<th class="head"><p>Sq. Ft.</p></th>
<th class="head"><p>Rating - Good</p></th>
<th class="head"><p>Rating - Bad</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1000</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1500</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>700</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1200</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="stacking">
<h2><span class="section-number">10.2. </span>Stacking<a class="headerlink" href="#stacking" title="Permalink to this heading">#</a></h2>
<p><strong>Stacking</strong> involves taking the predictions of various model types and combining them into an overall model by weighting their responses. For a concrete example, we might train three different models on a training set such as:</p>
<ul class="simple">
<li><p>A Logistic Regression Model</p></li>
<li><p>A Decision Tree Model</p></li>
<li><p>A Neural Network</p></li>
</ul>
<p>Each model will make a prediction <span class="math notranslate nohighlight">\(\hat{y}_j\)</span> and these outputs are used as an input for another model type (usually Linear Regression for regression and Logistic Regression for classification) to synthesize an overall answer based on weighting their individual guesses.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../../_images/stacking.png"><img alt="A visual depiction of stacked models. The inputs go to three models (Decision Tree, Logistic Regression, Neural Networks) which go to the Stacked Model with various weights." src="../../_images/stacking.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.1 </span><span class="caption-text">A Stacked model with 3 models in the ensemble each with weight <span class="math notranslate nohighlight">\(w_1, w_2, w_3\)</span></span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We won’t have much to say about this particular ensemble, as it is often much more of a heuristic of which models to use and learning the weights between them is simply training a model using their outputs as the overall-models inputs.</p>
</section>
<section id="bagging-random-forests">
<h2><span class="section-number">10.3. </span>Bagging - Random Forests<a class="headerlink" href="#bagging-random-forests" title="Permalink to this heading">#</a></h2>
<p>A <strong>Random Forest</strong> is a specific type of ensemble model that leverages the concept of <strong>bagging</strong>. Let’s first discuss the idea of this specific model before defining what bagging means in general. A Random Forest is an ensemble model composed of <span class="math notranslate nohighlight">\(T\)</span> Decision Trees. Each Decision Tree casts a “vote” for a particular label, and the ensemble combines all of the votes to make an overall prediction. For classification tasks, the votes are counted and the majority label is predicted. For regression the average label is predicted.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../../_images/random_forest.png"><img alt="A visual depiction of a random forest. Each tree casts a vote and the majority class is the overall ensemble's prediction" src="../../_images/random_forest.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.2 </span><span class="caption-text">A Random Forest, where each tree gets to cast a vote for the predicted label.</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A natural question is if we only have one training set, how can we learn a collection of <span class="math notranslate nohighlight">\(T\)</span> trees. Clearly each tree would need to be different somehow. If they were all the exact same model and all made the exact same predictions, an ensemble of clones would make the exact same decisions as the individuals. So how can we create differences in the tree?</p>
<p>We accomplish this by creating sampled datasets by <strong>bootstrapping</strong> our original dataset. Bootstrapping is the process of randomly sampling from our dataset, with replacement, to make new versions of our dataset that are slightly different than the original. We make a bootstrapped version of our dataset for each of the <span class="math notranslate nohighlight">\(T\)</span> trees we want to train, such that each dataset each tree trains on is now a random sample (with replacement) of our original dataset. A couple of key details about this sampling procedure.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>2. Think of a simplified example of randomly pulling three marbles from a bag that contains one red marble, one white marble, and one blue marble. If you draw the three marbles, replacing them after you pull them out, it’s entirely possible to draw the red one twice and the blue one once; thus leaving the white marble out of your set of 3 you drew.</p>
</aside>
<ul class="simple">
<li><p>Each bootstrapped dataset will have <span class="math notranslate nohighlight">\(n\)</span> examples like the original dataset. But because we sample with replacement, some examples from the original will be left out since we will not choose some by chance<sup>2</sup>.</p></li>
<li><p>We also select some number of features <span class="math notranslate nohighlight">\(m &lt; D\)</span> to randomly select that number of features for each bootstrapped sample. That means each tree is only trained on a subset of sized <span class="math notranslate nohighlight">\(m\)</span> of our original features (Not shown in the figure below). We’ll discuss this hyperparameter later.</p></li>
</ul>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../../_images/bootstrapping.png"><img alt="Bootstrapping data for each tree by randomly selecting examples with replacement. Not shown: Also would randomly select features." src="../../_images/bootstrapping.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.3 </span><span class="caption-text">Bootstrapping data for each tree by randomly selecting examples with replacement. Not shown: Also would randomly select features.</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Random Forests are a specific type of ensemble model known as a <strong>Bagging Ensemble</strong>. Bagging stands for “Bootstrapped aggregation” which comes from the fact that we are aggregating predictions over an ensemble of models trained on bootstrapped datasets.</p>
<p>So far, we have discussed our Random Forest is a collection of Decision Trees trained on random bootstraps of the original data. One important, but counter-intuitive, detail about these trees is we will also train them to grow <em>without any limit on how tall they can become</em>. This is precisely because we actually <em>want</em> each tree to overfit on the dataset it is trained on. It’s weird that after a whole book of trying to prevent overfitting, we are actually going to encourage overfitting in this context.</p>
<p>The “why” of this comes from the property that models that overfit are generally high variance. Recall from our bias-variance tradeoff, that high variance means a model will change wildly even to minor changes in the data (thus likely to overfit). Also from that discussion, models that have high variance generally also have low bias. Remember that we defined low bias to mean that on average (over all models you could learn from all possible versions of your dataset), we will be learning the true function. This fact is exactly what we are taking advantage of in an ensemble like a Random Forest! By letting each tree overfit and averaging over all of them, our hope is that this “average model” will be close to the true function since the individual overfit trees have low bias.</p>
<section id="random-forest-algorithm">
<h3><span class="section-number">10.3.1. </span>Random Forest Algorithm<a class="headerlink" href="#random-forest-algorithm" title="Permalink to this heading">#</a></h3>
<div class="proof algorithm admonition" id="random_forest_training">
<p class="admonition-title"><span class="caption-number">Algorithm 10.1 </span> (Random Forest Training)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: Training Dataset <span class="math notranslate nohighlight">\(D\)</span>. Hyperparameters: <span class="math notranslate nohighlight">\(T\)</span> number of trees and <span class="math notranslate nohighlight">\(m\)</span> number of features to select for each sample</p>
<p><strong>Output</strong>: An ensemble of <span class="math notranslate nohighlight">\(T\)</span> trees <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="arabic simple">
<li><p>For <span class="math notranslate nohighlight">\(i \in [1, ... T]\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(D' = bootstrap(D, m)\)</span> to randomly sample (with replacement) <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(m\)</span> columns from <span class="math notranslate nohighlight">\(D\)</span></p></li>
<li><p>Train a tree <span class="math notranslate nohighlight">\(\hat{t}_i\)</span> with no height limit on the data <span class="math notranslate nohighlight">\(D\)</span> (only with <span class="math notranslate nohighlight">\(m\)</span> features. The hope is these trees will overfit to the bootstrapped samples they are trained on.</p>
<ul class="simple">
<li><p>Note that each tree needs to remember which <span class="math notranslate nohighlight">\(m\)</span> features it trained on for prediction later on</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Return ensemble of <span class="math notranslate nohighlight">\(T\)</span> trees</p></li>
</ol>
</section>
</div><div class="proof algorithm admonition" id="random_forest_prediction">
<p class="admonition-title"><span class="caption-number">Algorithm 10.2 </span> (Random Forest Prediction)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: An ensemble of <span class="math notranslate nohighlight">\(T\)</span> trained trees <span class="math notranslate nohighlight">\(\hat{F}\)</span>, and an input <span class="math notranslate nohighlight">\(x\)</span> to make a prediction on</p>
<p><strong>Output</strong>: A prediction <span class="math notranslate nohighlight">\(\hat{y} = \hat{F}(x)\)</span> for input <span class="math notranslate nohighlight">\(x\)</span></p>
<ol class="arabic simple">
<li><p>For <span class="math notranslate nohighlight">\(\hat{t}_i \in \hat{F}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(x' = x\)</span> with only the <span class="math notranslate nohighlight">\(m\)</span> features <span class="math notranslate nohighlight">\(t_i\)</span> trained on</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i = \hat{t}_i(x)\)</span></p></li>
</ol>
</li>
<li><p>Return aggregate of all <span class="math notranslate nohighlight">\(\hat{y}_i\)</span></p>
<ul class="simple">
<li><p>For regression: <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{T} \sum_{i=1}^T \hat{y}_i\)</span></p></li>
<li><p>For classification: <span class="math notranslate nohighlight">\(\hat{y} = majority(\{\hat{y}_i\}_{i=1}^n)\)</span></p></li>
</ul>
</li>
</ol>
</section>
</div></section>
<section id="random-forest-properties">
<h3><span class="section-number">10.3.2. </span>Random Forest Properties<a class="headerlink" href="#random-forest-properties" title="Permalink to this heading">#</a></h3>
<p>When thinking about Random Forests, the following bullet points of important properties this model has in general is useful to keep in mind. Like always, these are just generalizations and it doesn’t mean you always should/shouldn’t use these models in a specific context. But they are useful rules of thumb to know.</p>
<ul class="simple">
<li><p>Random Forests use overfitting to their advantage by averaging out over many overfit trees. Averaging is a great variance reduction technique in general that this ensemble employs.</p></li>
</ul>
<div class="full-width docutils">
<aside class="sidebar">
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../../_images/kinect.png"><img alt="Screenshot of a paper showing how to highlight pose estimates from a 3D scan of body position" src="../../_images/kinect.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.4 </span><span class="caption-text"><a class="reference external" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/BodyPartRecognition.pdf">Example paper</a> using Random Forests to estimate poses from a depth image</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</aside>
<ul class="simple">
<li><p>Random Forests are versatile models that generally work pretty well in many contexts. Can use them in all sorts of settings such as regression, classification, clustering (not discussed), identifying important features.</p></li>
<li><p>Random Forests are generally low maintenance models. That means you actually don’t need to do much careful hyperparameter tuning (except for the number of features to sample <span class="math notranslate nohighlight">\(m\)</span>). Because we are averaging over all of these trees, we generally see improved generalization accuracy as we increase the number of trees <span class="math notranslate nohighlight">\(T\)</span>. More trees will take longer to train, but we don’t really need to worry about overfitting by adding too many trees to our ensemble. While there are some hyperparameters to tune, they generally have a smaller effect on the model’s performance. Because of this, Random Forests are often seen as a good “out of the box” model.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Random Forests are pretty efficient to train. Because we are sampling only <span class="math notranslate nohighlight">\(m\)</span> features, training each tree is generally pretty fast (for reasonable <span class="math notranslate nohighlight">\(m &lt;&lt; d\)</span>). In addition, the training of each tree is independent of the other. This means we can leverage concepts of <em>parallelization</em> to train trees on different CPUs to speed up our training time. If you wanted, you could buy a bunch of compute time from Amazon Web Services (AWS) and train one tree on each of the computers you rent to save you a lot of time!</p></li>
</ul>
</section>
<section id="random-forest-code">
<h3><span class="section-number">10.3.3. </span>Random Forest Code<a class="headerlink" href="#random-forest-code" title="Permalink to this heading">#</a></h3>
<p>Training a Random Forest in scikit-learn is quite easy, and is similar to many of the models we have seen previously. The following code block shows how to train a Random Forest classifier on the income dataset we saw in the last chapter (<a class="reference download internal" download="" href="../../_downloads/7286ec0f5e582206ad2d7c83f5c5fbe4/income.csv"><span class="xref download myst">income.csv</span></a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Load in data, and separate features and label</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;income.csv&quot;</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;income&quot;</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="n">label</span><span class="p">]</span>

<span class="c1"># Train test split</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Transform categorical features. Note that we use the same transformation</span>
<span class="c1"># on both train and test</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="c1"># Train model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train Accuracy: 1.0
Test  Accuracy: 0.8593582066635959
</pre></div>
</div>
</div>
</div>
</section>
<section id="practicalities">
<h3><span class="section-number">10.3.4. </span>Practicalities<a class="headerlink" href="#practicalities" title="Permalink to this heading">#</a></h3>
<p>We conclude this section with a few practical details about using Random Forests.</p>
<section id="number-of-features-m">
<h4><span class="section-number">10.3.4.1. </span>Number of features <span class="math notranslate nohighlight">\(m\)</span><a class="headerlink" href="#number-of-features-m" title="Permalink to this heading">#</a></h4>
<p>While adding more trees will generally improve the Random Forest model, that doesn’t mean you don’t have to do any hyperparameter tuning. Recall that in our bootstrapping step, we randomly select <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(m\)</span> features, where <span class="math notranslate nohighlight">\(n\)</span> is the number of training points in the original dataset and <span class="math notranslate nohighlight">\(m\)</span> is a hyperparameter. Randomly selecting features is important for the overall ensemble being better since it de-correlates all of the trees in the ensemble. In general, you have to tune this value for <span class="math notranslate nohighlight">\(m\)</span> by comparing the performance of various models with different settings on <span class="math notranslate nohighlight">\(m\)</span> with a validation set or cross validation. The authors of the original Random Forest paper suggest <span class="math notranslate nohighlight">\(m = \sqrt{D}\)</span> features for each sample in regression tasks and <span class="math notranslate nohighlight">\(m = \lfloor D/3 \rfloor\)</span> for classification tasks. It’s unclear how effective those actually are, but they are starting points that you can then tune from.</p>
</section>
<section id="out-of-bag-oob-error">
<h4><span class="section-number">10.3.4.2. </span>Out of Bag (OOB) Error<a class="headerlink" href="#out-of-bag-oob-error" title="Permalink to this heading">#</a></h4>
<p>One useful feature of Random Forests is that you can actually get an estimate of future performance of the ensemble without a test set and only using the training set! While that seems surprising, we are able to do this by relying on the fact that not every tree in the ensemble saw every example from the training set. Because each tree was trained on a bootstrapped sample of the training sets, some of the examples will be left out from each trees bootstrapped dataset. If we want to estimate future error, we can do so by asking each tree to make predictions on the training points it <em>didn’t</em> train on.</p>
<p>This concept is called the <strong>Out of Bag Error</strong> (OOB Error). To calculate it we have the ensemble make a prediction on every training point, but only allow the trees that didn’t train on that training point to have a vote in the decision. Then we just collect all of these ensemble predictions over our training set and compute whichever quality metric we are using on the predictions and true labels.</p>
</section>
</section>
</section>
<section id="boosting-adaboost">
<h2><span class="section-number">10.4. </span>Boosting - AdaBoost<a class="headerlink" href="#boosting-adaboost" title="Permalink to this heading">#</a></h2>
<p>In this last section of the chapter, we will introduce another type of ensemble model that is also a collection of trees, but the details of how it works are quite different. We will explore a particular learning algorithm called <strong>AdaBoost</strong> that is an example of our final type of ensemble, a <strong>boosted ensemble</strong>. Before defining what these terms are, let’s explore a bit of the history that led to the model’s inception.</p>
<p>Many machine learning researchers aer interested in the theoretical limits of ML models: what they can or can’t learn, what guarantees we can make about a model’s performance, and many other important questions. One example problem was asking if an ensemble could be constructed from ineffective models to make a better ensemble. A <strong>weak learner</strong> is a model that does only slightly better than random guessing at a task. Kearns and Valient (1988, 1989) asked if a set of weak learners in an ensemble could be created to make a strong learner. In 1990, Schapire found that this could be done with a model he called the AdaBoost model.</p>
<p><strong>AdaBoost</strong> is an ensemble of decision trees much like Random Forests, that has three notable differences that impact how we train it. We’ll present these differences at a high-level before diving into details with an example.</p>
<ol class="arabic simple">
<li><p>Instead of using tall decision trees that overfit to the data, we will limit the models in the ensemble to <em>decision stumps</em> (one branch).</p></li>
<li><p>Instead of doing a majority vote over the models in the ensemble, each model will be assigned a weight and we take a <em>weighted majority vote</em>. For example, if we are in a binary classification setting where <span class="math notranslate nohighlight">\(y \in \{+1, -1\}, we will make predictions as follows where \)</span>\hat{f}_t(x)<span class="math notranslate nohighlight">\( is the prediction of decision stump \)</span>t<span class="math notranslate nohighlight">\( and \)</span>\hat{w}_t$ is that models weight for the majority vote.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\hat{y} = \hat{F}(x) = sign\left( \sum_{t=1}^T \hat{w}_t \hat{f}_t(x) \right)\]</div>
<ol class="arabic simple" start="3">
<li><p>Instead of bootstrapping datasets for each model in the ensemble, we will use the whole dataset to train each decision stump. To add variation between stumps, we will add a notion of <em>datapoint weights</em> <span class="math notranslate nohighlight">\(\alpha_i\)</span> and find decision stumps that minimize a notion of <em>weighted classification error</em>.</p></li>
</ol>
<section id="adaboost-predictions">
<h3><span class="section-number">10.4.1. </span>AdaBoost Predictions<a class="headerlink" href="#adaboost-predictions" title="Permalink to this heading">#</a></h3>
<p>Before discussing all of the details behind these differences, let’s see an example AdaBoost model and how it makes predictions. In the figure below, we have an AdaBoost model with four decision stumps and which each model predicts on some example point <span class="math notranslate nohighlight">\(x\)</span>.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="../../_images/adaboost.png"><img alt="Four decision stumps in an AdaBoost model with various model weights (explained below)" src="../../_images/adaboost.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.5 </span><span class="caption-text">Example AdaBoost ensemble</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To find the ensemble’s prediction for the input <span class="math notranslate nohighlight">\(x\)</span>, we have to first get the prediction from each stump in the ensemble and then combine their predictions with their weights to get a weighted majority vote.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{y} &amp;= \hat{F}(x)\\
 &amp;= sign\left( \sum_{t=1}^T \hat{w}_t \hat{f}_t(x) \right)\\
 &amp;= sign\left(2 \cdot 1  + (-1) \cdot (-1) + 1.5 \cdot (-1) + 0 \cdot (-1)\right)\\
 &amp;= sign(1.5)\\
 &amp;= +1 \end{split}\]</div>
</section>
<section id="adaboost-training">
<h3><span class="section-number">10.4.2. </span>AdaBoost Training<a class="headerlink" href="#adaboost-training" title="Permalink to this heading">#</a></h3>
<p>So as we mentioned before, our training procedure for AdaBoost is going to be quite different than it was for Random Forests. In AdaBoost, we will be training each model <em>in succession</em>, where we will use the errors the previous model made to influence how the next model is trained. This process is a specific example of the general concept of <strong>boosting</strong> in ensembles, where future models are trained based on the results of previous models.</p>
<p>To do this, we will keep track of two sets of weights for AdaBoost:</p>
<ul class="simple">
<li><p>Model weights <span class="math notranslate nohighlight">\(\hat{w}_t\)</span> that we will use to weight the predictions from each model. These are the weights discussed in the last section. The intuition for how we will compute these weights is that a more accurate model in our ensemble should have a higher weight in the ensemble.</p></li>
<li><p>Dataset weights <span class="math notranslate nohighlight">\(\alpha_i\)</span> that will influence how we train each model. The intuition is that we will want to put more emphasis on examples with more weight, and put more weight on examples that are often misclassified.</p></li>
</ul>
<p>So at a high-level, our AdaBoost training algorithm will have the following steps. We will repeat this algorithm at the end of the section with all of the details filled in.</p>
<div class="proof algorithm admonition" id="adaboost_training_1">
<p class="admonition-title"><span class="caption-number">Algorithm 10.3 </span> (AdaBoost Training)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: Training Dataset <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n\times d}, y \in \{0, 1\}\)</span>. Hyperparameters: Number of trees <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Output</strong>: An AdaBoost ensemble of <span class="math notranslate nohighlight">\(T\)</span> trees <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="arabic simple">
<li><p>For <span class="math notranslate nohighlight">\(i \in [1, ... T]\)</span>:</p>
<ol class="arabic simple">
<li><p>Learn <span class="math notranslate nohighlight">\(\hat{f}_t(x) based on current dataset weights \)</span>\alpha_{i,t}$</p></li>
<li><p>Compute model weight <span class="math notranslate nohighlight">\(\hat{w}_t\)</span> for learned model</p></li>
<li><p>Update dataset weights <span class="math notranslate nohighlight">\(\alpha{i, t+1}\)</span></p></li>
</ol>
</li>
<li><p>Return AdaBoost ensemble with <span class="math notranslate nohighlight">\(T\)</span> trees and model weights <span class="math notranslate nohighlight">\(\{\hat{w}_t\}_{t=1}^T\)</span></p></li>
</ol>
</section>
</div><p>In the following sub-sections, we will explain each of these bullet points in detail.</p>
<section id="learning-from-weighted-data">
<h4><span class="section-number">10.4.2.1. </span>Learning from Weighted Data<a class="headerlink" href="#learning-from-weighted-data" title="Permalink to this heading">#</a></h4>
<p>A key part of the AdaBoost algorithm is associated a weight to each example in our training set. At a high level, we will be updating weights to increase the weights of examples we get wrong and decreasing the weights for examples we get right. But how do we utilize those weights?</p>
<table class="table" id="example-weights">
<caption><span class="caption-number">Table 10.3 </span><span class="caption-text">Example Dataset Weights for a small cancer dataset</span><a class="headerlink" href="#example-weights" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>TumorSize</p></th>
<th class="head"><p>IsSmoker</p></th>
<th class="head"><p>Malignant (y)</p></th>
<th class="head"><p>Weight</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Small</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>0.5</p></td>
</tr>
<tr class="row-odd"><td><p>Small</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>1.2</p></td>
</tr>
<tr class="row-even"><td><p>Large</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>0.3</p></td>
</tr>
<tr class="row-odd"><td><p>Large</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>0.5</p></td>
</tr>
<tr class="row-even"><td><p>Small</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>3.3</p></td>
</tr>
</tbody>
</table>
<p>Instead of finding a Decision Stump to minimize classification error, we we will find the Decision Stump that minimizes a <strong>weighted classification error</strong>. The intuition is we care about the fraction of the training dataset weight that we label incorrectly, and want to minimize this weighted error. Another intuition is if an example has weight <span class="math notranslate nohighlight">\(\alpha_{i, t} = 2\)</span>, then making a mistake on that example is twice as bad as making a mistake on an example with weight 1.</p>
<div class="math notranslate nohighlight">
\[WeightedError(f_t) = \frac{\sum_{i=1}^n \alpha_{i,t} \cdot \indicator{\hat{f_t}(x) \neq y_i}}{\sum_{i=1}^n \alpha_{i,t}}\]</div>
<p>So our decision stump learning algorithm is mostly the same, but now we try to find the stump with the lowest weighted error. That also means when it comes to deciding the prediction for a leaf node, we will predict the class with the largest weight at that leaf instead of the highest number of examples. Consider two possible splits for a decision stump on <a class="reference internal" href="#example-weights"><span class="std std-numref">Table 10.3</span></a>.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="../../_images/weighted_stumps.png"><img alt="Two decision stumps, one split on TumorSize and the other IsSmoker, and their predictions" src="../../_images/weighted_stumps.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.6 </span><span class="caption-text">Two possible Decision Stumps based on weighted error. Note that the right stump predicts No for both branches because each leaf node has more weight on the No class.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To compute the weighted error for each stump, we compute the fraction of the total weight that gets misclassified.</p>
<ul class="simple">
<li><p>For the first stump split on TumorSize, the weighted error is
$<span class="math notranslate nohighlight">\(\frac{0.3 + 1.2}{5.8} \approx 0.26\)</span>$</p></li>
<li><p>For the second stump split on IsSmoker, the weighted error is
$<span class="math notranslate nohighlight">\(\frac{1.7 + 0}{5.8} \approx 0.29\)</span>$</p></li>
</ul>
<p>Since the first stump has lower weighted error, that’s the one we would choose with these weights.</p>
<p>A similar procedure for finding the best split of a numeric feature can also be used, where we decide the threshold that has the lowest weighted error.</p>
</section>
<section id="model-weights-hat-w-t">
<h4><span class="section-number">10.4.2.2. </span>Model Weights <span class="math notranslate nohighlight">\(\hat{w}_t\)</span><a class="headerlink" href="#model-weights-hat-w-t" title="Permalink to this heading">#</a></h4>
<p>Now that we have a procedure to train one of the Decision Stumps in our ensemble, we can now compute the next weights outlined in <a class="reference internal" href="#adaboost_training_1">Algorithm 10.3</a>: <span class="math notranslate nohighlight">\(\hat{w}_t\)</span>.</p>
<p>Our intuition for these model weights was to assign more weight to models that are more accurate, and less weight to models that are less accurate. Without proof, we will just show that the following formula works well for model weights.</p>
<div class="math notranslate nohighlight">
\[\hat{w}_t = \frac{1}{2} \ln\left( \frac{1 - WeightedError(\hat{f}_t)}{WeightedError(\hat{f}_t)}\right)\]</div>
<p>If you plug in the weighted error from our last example (0.26), the model weight for that model would be</p>
<div class="math notranslate nohighlight">
\[\hat{w}_t = \frac{1}{2} \ln\left( \frac{1 - WeightedError(\hat{f}_t)}{WeightedError(\hat{f}_t)}\right) = \frac{1}{2}\ln\left(\frac{1 - 0.26}{0.26} \right) \approx 0.52\]</div>
<p>In the following plot, we show that this formula has the desired property of assigning more weight to more accurate models and less weight to less accurate models.</p>
<div class="cell tag_hide-input tag_remove-stderr docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">model_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">errors</span><span class="p">)</span> <span class="o">/</span> <span class="n">errors</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">model_weights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model Weight based on Weighted Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Weighted Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$\hat</span><span class="si">{w}</span><span class="s2">_t$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">));</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../_images/ad00dabc93077d30ece23b554477a2dbe78e7106f29af3a5ca6d984ceeb5db77.png"><img alt="../../_images/ad00dabc93077d30ece23b554477a2dbe78e7106f29af3a5ca6d984ceeb5db77.png" class="align-center" src="../../_images/ad00dabc93077d30ece23b554477a2dbe78e7106f29af3a5ca6d984ceeb5db77.png" style="width: 75%;" /></a>
</div>
</div>
<p>Note that the output of the model weights are unbounded and can range from <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>. One peculiar fact is that the model weights can in fact be negative. A negative model weight for a binary classification setting actually means we are <em>reversing</em> the predictions of that model. Consider an example of a model with <span class="math notranslate nohighlight">\(WeightedError = 0.99\)</span> makes mistakes on the vast majority of the weight examples. While that classifier sounds really bad, it’s actually a really good classifier if you just swap it’s predictions. If you just do the opposite of what it says, you will only have <span class="math notranslate nohighlight">\(WeightedError = 0.01\)</span>!</p>
</section>
<section id="computing-dataset-weights-alpha-i-t-1">
<h4><span class="section-number">10.4.2.3. </span>Computing Dataset Weights $\alpha_{i, t+1}<a class="headerlink" href="#computing-dataset-weights-alpha-i-t-1" title="Permalink to this heading">#</a></h4>
<p>For the last step of the AdaBoost training algorithm, we need to update the dataset weights for the next iteration so that the next iteration pays more attention to different examples. Our goal is that examples that we made mistakes on should get higher weight while examples that were predicted correctly should have lower weight. We will use the following update formula.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\alpha_{i, t+1} = \begin{cases}
        \alpha_{i, t}e^{-\hat{w}_t} &amp; \text{if } \hat{f}_t(x) = y_i\\
        \alpha_{i, t}e^{\hat{w}_t} &amp; \text{if } \hat{f}_t(x) \neq y_i\\
  \end{cases}\end{split}\]</div>
<p>This multiplicative weight update increases the weights for examples where the there was a misclassification and decreases the weights for examples where the classification was correct.</p>
<p>As a implementation note, we often normalize the dataset weights after updating them so they are all on the same scale. This isn’t a problem since weighted classification error is already a fraction of the total weight. But by having them all in the same scale, we avoid some numeric stability issues.</p>
<div class="math notranslate nohighlight">
\[\alpha_{i, t+1} \gets \frac{\alpha_{i, t+1}}{\sum_{j=1}^n \alpha_{j, t+1}}\]</div>
</section>
<section id="putting-it-all-together">
<h4><span class="section-number">10.4.2.4. </span>Putting it all Together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this heading">#</a></h4>
<div class="proof algorithm admonition" id="adaboost_training_2">
<p class="admonition-title"><span class="caption-number">Algorithm 10.4 </span> (AdaBoost Training (First Glance))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: Training Dataset <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n\times d}, y \in \{0, 1\}\)</span>. Hyperparameters: Number of trees <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Output</strong>: An AdaBoost ensemble of <span class="math notranslate nohighlight">\(T\)</span> trees <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="arabic">
<li><p>Initialize dataset weights <span class="math notranslate nohighlight">\(\alpha_{i, 1} = 1/n\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(i \in [1, ... T]\)</span>:</p>
<ol class="arabic">
<li><p>Learn <span class="math notranslate nohighlight">\(\hat{f}_t(x) based on current dataset weights \)</span>\alpha_{i,t}$ to find a stump that minimizes</p>
<p><span class="math notranslate nohighlight">\(WeightedError(f_t) = \frac{\sum_{i=1}^n \alpha_{i,t} \cdot \indicator{\hat{f_t}(x) \neq y_i}}{\sum_{i=1}^n \alpha_{i,t}}\)</span></p>
</li>
<li><p>Compute model weight <span class="math notranslate nohighlight">\(\hat{w}_t\)</span> for learned model</p>
<p><span class="math notranslate nohighlight">\(\hat{w}_t = \frac{1}{2} \ln\left( \frac{1 - WeightedError(\hat{f}_t)}{WeightedError(\hat{f}_t)}\right)\)</span></p>
</li>
<li><p>Update dataset weights <span class="math notranslate nohighlight">\(\alpha{i, t+1}\)</span></p>
<p><span class="math notranslate nohighlight">\(\alpha_{i, t+1} = \begin{cases}
         \alpha_{i, t}e^{-\hat{w}_t} &amp; \text{if } \hat{f}_t(x) = y_i\\
         \alpha_{i, t}e^{\hat{w}_t} &amp; \text{if } \hat{f}_t(x) \neq y_i\\
   \end{cases}\)</span></p>
</li>
<li><p>Normalize dataset weights</p>
<p><span class="math notranslate nohighlight">\(\alpha_{i, t+1} \gets \frac{\alpha_{i, t+1}}{\sum_{j=1}^n \alpha_{j, t+1}}\)</span></p>
</li>
</ol>
</li>
<li><p>Return AdaBoost ensemble with <span class="math notranslate nohighlight">\(T\)</span> trees and model weights <span class="math notranslate nohighlight">\(\{\hat{w}_t\}_{t=1}^T\)</span></p></li>
</ol>
</section>
</div></section>
</section>
<section id="adaboost-code">
<h3><span class="section-number">10.4.3. </span>AdaBoost code<a class="headerlink" href="#adaboost-code" title="Permalink to this heading">#</a></h3>
<p>Training an AdaBoost model with a library like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is very easy just like with Random Forests. Here is a similar code example that shows how to use the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library implementation of AdaBoost.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="c1"># Load in data, and separate features and label</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;income.csv&quot;</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;income&quot;</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="n">label</span><span class="p">]</span>

<span class="c1"># Train test split</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Transform categorical features. Note that we use the same transformation</span>
<span class="c1"># on both train and test</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="c1"># Train model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train Accuracy: 0.8758829852579852
Test  Accuracy: 0.8708736373407032
</pre></div>
</div>
</div>
</div>
</section>
<section id="adaboost-walkthrough">
<h3><span class="section-number">10.4.4. </span>AdaBoost Walkthrough<a class="headerlink" href="#adaboost-walkthrough" title="Permalink to this heading">#</a></h3>
<p>To understand this process more concretely, let’s walk through the steps to train an AdaBoost model. The exact code for this section is not as important to understand, but it is important to understand the computations we are doing and their results.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>



<span class="n">X_LIM</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">Y_LIM</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">pos_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;positive&quot;</span><span class="p">]</span>
    <span class="n">neg_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;negative&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">dataset_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">pos_s</span> <span class="o">=</span> <span class="mi">200</span>
      <span class="n">neg_s</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pos_weights</span> <span class="o">=</span> <span class="n">dataset_weights</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;positive&quot;</span><span class="p">]</span>
      <span class="n">neg_weights</span> <span class="o">=</span> <span class="n">dataset_weights</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;negative&quot;</span><span class="p">]</span>

      <span class="c1"># Hard coding some numbers to scale weights to meaningful sizes</span>
      <span class="n">effective_weight_max</span> <span class="o">=</span> <span class="mf">0.1</span>
      <span class="n">start_s</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">effective_max_s</span> <span class="o">=</span> <span class="mi">200</span>
      <span class="n">pos_s</span> <span class="o">=</span> <span class="n">start_s</span> <span class="o">+</span> <span class="n">effective_max_s</span> <span class="o">*</span> <span class="n">pos_weights</span> <span class="o">/</span> <span class="n">effective_weight_max</span>
      <span class="n">neg_s</span> <span class="o">=</span> <span class="n">start_s</span> <span class="o">+</span> <span class="n">effective_max_s</span> <span class="o">*</span> <span class="n">neg_weights</span> <span class="o">/</span> <span class="n">effective_weight_max</span>


    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">pos_data</span><span class="p">[</span><span class="s2">&quot;feature1&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">pos_data</span><span class="p">[</span><span class="s2">&quot;feature2&quot;</span><span class="p">],</span>
                <span class="n">c</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">pos_s</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">neg_data</span><span class="p">[</span><span class="s2">&quot;feature1&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">neg_data</span><span class="p">[</span><span class="s2">&quot;feature2&quot;</span><span class="p">],</span>
                <span class="n">c</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">neg_s</span><span class="p">)</span> <span class="c1">#weight_scale * neg_weights)</span>

    <span class="k">if</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">[[</span><span class="s2">&quot;feature1&quot;</span><span class="p">,</span> <span class="s2">&quot;feature2&quot;</span><span class="p">]],</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
            <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;feature1&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;feature2&quot;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">],</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature2&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">X_LIM</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">Y_LIM</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AnimlAdaBoost</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">models</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span> <span class="k">if</span> <span class="n">models</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_weights</span> <span class="o">=</span> <span class="n">model_weights</span> <span class="k">if</span> <span class="n">model_weights</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted_</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="s2">&quot;positive&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">add_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">model_weight</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unable to add trees if limit is set. Set to None first&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">NotImplemented</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">limit_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">i</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1"># Assumes binary positive/negative predictions</span>
        <span class="n">total_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

        <span class="n">limit</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">limit</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="s2">&quot;positive&quot;</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">])</span>
            <span class="n">total_preds</span> <span class="o">=</span> <span class="n">total_preds</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">preds</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;positive&quot;</span> <span class="k">if</span> <span class="n">total_pred</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;negative&quot;</span> <span class="k">for</span> <span class="n">total_pred</span> <span class="ow">in</span> <span class="n">total_preds</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We start by loading in our <a class="reference download internal" download="" href="../../_downloads/844c53e00d63017508cd7a7c8d399396/synthetic.csv"><span class="xref download myst">synthetic dataset</span></a> as an example and plot out the points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Read in data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./synthetic.csv&quot;</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;feature1&quot;</span><span class="p">,</span> <span class="s2">&quot;feature2&quot;</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="adaboost-data">
<a class="reference internal image-reference" href="../../_images/a0a93153c7732d7d92dea7bdd7c9f6246821c12882b78b3438d5fc80b9dd8f05.png"><img alt="Synthetic data with positive/negative labels" class="align-center" src="../../_images/a0a93153c7732d7d92dea7bdd7c9f6246821c12882b78b3438d5fc80b9dd8f05.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.7 </span><span class="caption-text">Example synthetic dataset</span><a class="headerlink" href="#adaboost-data" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>We then training a decision stump that minimizes the weighted classification error. To start, the <code class="docutils literal notranslate"><span class="pre">dataset_weight</span></code> (<span class="math notranslate nohighlight">\(\alpha_{i, 1}\)</span>) start off as equal weight on each data point. Note that the size of each data point corresponds to its weight, in this plot they are all the same. We can see the decision boundary for this stump. After training this decision stump, we calculate the model’s weighted error to compute the model weight and updated dataset weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">animl_train_stump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">original_dataset_weights</span> <span class="o">=</span> <span class="n">dataset_weights</span>  <span class="c1"># Save for later for printing</span>

    <span class="c1"># Train the weak classifier on the data with the current weights</span>
    <span class="n">stump</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">stump</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">dataset_weights</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">stump</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Calculate the weighted error</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">!=</span> <span class="n">labels</span>
    <span class="n">weighted_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dataset_weights</span><span class="p">,</span> <span class="n">errors</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dataset_weights</span><span class="p">)</span>

    <span class="c1"># Calculate the model weight</span>
    <span class="n">model_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weighted_error</span><span class="p">)</span> <span class="o">/</span> <span class="n">weighted_error</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="c1"># Update dataset weights</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">errors</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># 1 for error, -1 for correct</span>
    <span class="n">dataset_weights</span> <span class="o">=</span>  <span class="n">dataset_weights</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model_weight</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">)</span>

    <span class="c1"># Normalize dataset weights</span>
    <span class="n">dataset_weights</span> <span class="o">=</span> <span class="n">dataset_weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dataset_weights</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial Dataset Weights:&quot;</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">original_dataset_weights</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">()</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weighted Error:&quot;</span><span class="p">,</span> <span class="n">weighted_error</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Weight  :&quot;</span><span class="p">,</span> <span class="n">model_weight</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">()</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New Dataset Weights:&quot;</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_weights</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">stump</span><span class="p">,</span> <span class="n">model_weight</span><span class="p">,</span> <span class="n">dataset_weights</span>

<span class="c1"># Initialize the dataset weights to be uniform</span>
<span class="n">dataset_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))])</span>

<span class="c1"># Train a decision stump</span>
<span class="n">tree1</span><span class="p">,</span> <span class="n">model_weight1</span><span class="p">,</span> <span class="n">new_dataset_weights</span> <span class="o">=</span> <span class="n">animl_train_stump</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">dataset_weights</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot with original weights that were trained on</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="o">=</span><span class="n">dataset_weights</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">tree1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial Dataset Weights:
[0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333
 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333
 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333
 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333
 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333]

Weighted Error: 0.2
Model Weight  : 0.6931471805599453

New Dataset Weights:
[0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.08333333
 0.02083333 0.08333333 0.08333333 0.02083333 0.02083333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.02083333 0.02083333]
</pre></div>
</div>
<figure class="align-default" id="adaboost-iter-1">
<a class="reference internal image-reference" href="../../_images/823c0e2e5f138e79e0d5e58e67e88866cfc3722bd06d6d0fcb5cd79fb9d7eb89.png"><img alt="Plot of decision boundary stump over data (described above)" class="align-center" src="../../_images/823c0e2e5f138e79e0d5e58e67e88866cfc3722bd06d6d0fcb5cd79fb9d7eb89.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.8 </span><span class="caption-text">First Decision Stump learned by AdaBoost</span><a class="headerlink" href="#adaboost-iter-1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>After this step, the dataset weights now look like the following. Note that the first stump made 6 mistakes, so those 6 points are now larger while the remaining ones are smaller.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_weights</span> <span class="o">=</span> <span class="n">new_dataset_weights</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="o">=</span><span class="n">dataset_weights</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="../../_images/89a6862dd055242f6321a575d905ce1bb1d2e889bf0a38728a1175b14d208eb6.png"><img alt="Dataset weights after update" class="align-center" src="../../_images/89a6862dd055242f6321a575d905ce1bb1d2e889bf0a38728a1175b14d208eb6.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.9 </span><span class="caption-text">First Decision Stump learned by AdaBoost</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>One the next iteration when we learn the next stump, we now use the new dataset weights and find the next stump that has the lowest weighted classification error. Again, we follow the steps to train the stump, calculate its weighted error, use that to calculate the model weight, and then update the dataset weights for the next iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train a decision stump</span>
<span class="n">tree2</span><span class="p">,</span> <span class="n">model_weight2</span><span class="p">,</span> <span class="n">new_dataset_weights</span> <span class="o">=</span> <span class="n">animl_train_stump</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">dataset_weights</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot with original weights that were trained on</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="o">=</span><span class="n">dataset_weights</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">tree2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial Dataset Weights:
[0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.08333333
 0.02083333 0.08333333 0.08333333 0.02083333 0.02083333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.08333333 0.02083333
 0.02083333 0.02083333 0.02083333 0.02083333 0.02083333 0.02083333]

Weighted Error: 0.1458333333333333
Model Weight  : 0.8838309588244975

New Dataset Weights:
[0.01219512 0.01219512 0.01219512 0.01219512 0.04878049 0.01219512
 0.01219512 0.01219512 0.01219512 0.01219512 0.04878049 0.04878049
 0.07142857 0.04878049 0.04878049 0.01219512 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.01219512 0.04878049 0.01219512
 0.01219512 0.01219512 0.01219512 0.01219512 0.07142857 0.01219512]
</pre></div>
</div>
<figure class="align-default" id="adaboost-iter-2">
<a class="reference internal image-reference" href="../../_images/c02979ef6c8bf1712318d69117838ed5cc6f91617c36a545533bd83126143118.png"><img alt="../../_images/c02979ef6c8bf1712318d69117838ed5cc6f91617c36a545533bd83126143118.png" class="align-center" src="../../_images/c02979ef6c8bf1712318d69117838ed5cc6f91617c36a545533bd83126143118.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.10 </span><span class="caption-text">Decision stump trained on iteration 2</span><a class="headerlink" href="#adaboost-iter-2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>We can continue this process for another stump after updating all of our weights again. The following plot shows the decision boundary of the new stump and the dataset weights it trained on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_weights</span> <span class="o">=</span> <span class="n">new_dataset_weights</span>
<span class="c1"># Train a decision stump</span>
<span class="n">tree3</span><span class="p">,</span> <span class="n">model_weight3</span><span class="p">,</span> <span class="n">new_dataset_weights</span> <span class="o">=</span> <span class="n">animl_train_stump</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">dataset_weights</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot with original weights that were trained on</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset_weights</span><span class="o">=</span><span class="n">dataset_weights</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">tree3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial Dataset Weights:
[0.01219512 0.01219512 0.01219512 0.01219512 0.04878049 0.01219512
 0.01219512 0.01219512 0.01219512 0.01219512 0.04878049 0.04878049
 0.07142857 0.04878049 0.04878049 0.01219512 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.01219512 0.04878049 0.01219512
 0.01219512 0.01219512 0.01219512 0.01219512 0.07142857 0.01219512]

Weighted Error: 0.2682926829268292
Model Weight  : 0.5016510544318926

New Dataset Weights:
[0.00833333 0.00833333 0.00833333 0.00833333 0.09090909 0.00833333
 0.00833333 0.00833333 0.00833333 0.02272727 0.03333333 0.09090909
 0.04880952 0.09090909 0.09090909 0.00833333 0.04880952 0.04880952
 0.04880952 0.04880952 0.04880952 0.02272727 0.09090909 0.00833333
 0.00833333 0.00833333 0.00833333 0.00833333 0.04880952 0.00833333]
</pre></div>
</div>
<figure class="align-default" id="adaboost-iter-3">
<a class="reference internal image-reference" href="../../_images/2ec4272eb1cfbeabaa4ac5e2848454d988f001e373d32f360381a504d737864e.png"><img alt="../../_images/2ec4272eb1cfbeabaa4ac5e2848454d988f001e373d32f360381a504d737864e.png" class="align-center" src="../../_images/2ec4272eb1cfbeabaa4ac5e2848454d988f001e373d32f360381a504d737864e.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.11 </span><span class="caption-text">Decision stump trained on iteration 3</span><a class="headerlink" href="#adaboost-iter-3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>At this point, our AdaBoost ensemble has two trees in it. We can compute a similar decision boundary, but for the whole ensemble. Recall that the predictions for AdaBoost follow the formula</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \hat{F}(x) = sign\left(\sum_{t=1}^T \hat{w}_t \hat{f}_t(x)\right)\]</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="nn">sym</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">sign</span> <span class="o">=</span> <span class="n">sym</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="s2">&quot;sign&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">f1</span> <span class="o">=</span> <span class="n">sym</span><span class="o">.</span><span class="n">symbols</span><span class="p">(</span><span class="s2">&quot;\hat</span><span class="si">{f}</span><span class="s2">_1(x)&quot;</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;sympy&#39;
</pre></div>
</div>
</div>
</div>
<p>So the following decision boundary is found by the following function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">adaboost</span> <span class="o">=</span> <span class="n">AnimlAdaBoost</span><span class="p">(</span>
  <span class="p">[</span><span class="n">tree1</span><span class="p">,</span> <span class="n">tree2</span><span class="p">,</span> <span class="n">tree3</span><span class="p">],</span>
  <span class="p">[</span><span class="n">model_weight1</span><span class="p">,</span> <span class="n">model_weight2</span><span class="p">,</span> <span class="n">model_weight3</span><span class="p">])</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">adaboost</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can let this process continue for as many trees as we would like to add <span class="math notranslate nohighlight">\(T\)</span>. Suppose we set <span class="math notranslate nohighlight">\(T = 250\)</span>. The following video shows the result decision boundary of the <em>ensemble</em> (not the individual trees), as we add more and more tress to the ensemble. By the end, it is able to perfectly classify the training set.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define helper code to make a video</span>

<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>


<span class="k">def</span> <span class="nf">animl_animate_adaboost</span><span class="p">(</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">checkpoints</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># function takes frame as an input</span>
    <span class="k">def</span> <span class="nf">animation_func</span><span class="p">(</span><span class="n">frame</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">ensemble</span><span class="o">.</span><span class="n">limit_to</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">[</span><span class="n">frame</span><span class="p">])</span>
        <span class="n">animl_plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AdaBoost Decision Boundary with Num Trees = </span><span class="si">{</span><span class="n">checkpoints</span><span class="p">[</span><span class="n">frame</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">anim_created</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animation_func</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">750</span><span class="p">)</span>
    <span class="n">anim_created</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;../../_static/classification/ensembles/adaboost_decisions.mp4&quot;</span><span class="p">,</span>
                      <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;ffmpeg&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_TREES</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">CHECKPOINTS</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">))</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">MAX_TREES</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">ensemble</span> <span class="o">=</span> <span class="n">AnimlAdaBoost</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_TREES</span><span class="p">):</span>
    <span class="n">tree</span><span class="p">,</span> <span class="n">model_weight</span><span class="p">,</span> <span class="n">dataset_weights</span> <span class="o">=</span> <span class="n">animl_train_stump</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">dataset_weights</span><span class="p">)</span>

    <span class="n">ensemble</span><span class="o">.</span><span class="n">add_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">model_weight</span><span class="p">)</span>

<span class="n">animl_animate_adaboost</span><span class="p">(</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">CHECKPOINTS</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<!-- Might need to save in the static folder -->
<video controls="True" preload="auto"><source src="../../_static/classification/ensembles/adaboost_decisions.mp4" type="video/mp4"></video><p>If you think about it for a second, let’s think of how incredible this is. Remember that the ensemble members are incredibly simple models (decision stumps). Despite their over-simplicity, we were able to make a complex enough ensemble to learn a more complex function.</p>
<p>It turns out, this is not just a coincidence working with our small example dataset, but a general statement of the power of AdaBoost. The <strong>AdaBoost training theorem</strong> states that under some reasonable assumptions (the existence of a weak learner for the task) our AdaBoost model will have training error that approaches 0 as the number of trees <span class="math notranslate nohighlight">\(T \rightarrow \infty\)</span>. This is a surprising result from the fact the individual weak learners aren’t that powerful.</p>
</section>
<section id="overfitting">
<h3><span class="section-number">10.4.5. </span>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this heading">#</a></h3>
<p>While the AdaBoost training theorem is promising, we should remember that achieving 0 training error is likely a sign that our model is overfitting! And that’s definitely the case here. While we can achieve lower and lower training error, we will see the same idea of our generalization error improving at first before getting worse. The following graph shows the training/test <em>accuracy</em> on our example income dataset from earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">learning_curve</span><span class="p">(</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">staged_predictions</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">predictions</span> <span class="ow">in</span> <span class="n">staged_predictions</span><span class="p">:</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracies</span>


<span class="n">NUM_TREES</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">NUM_TREES</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

<span class="n">num_trees</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">NUM_TREES</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">train_accs</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">test_accs</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">test_accs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Trees&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy (clipped)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>How do you choose the number of trees <span class="math notranslate nohighlight">\(T\)</span> then? Just like any other hyperparameter! Use a validation set or cross validation to choose the best <span class="math notranslate nohighlight">\(T\)</span>. Recall that you cannot choose hyperparameters based on train or test performance, you need to use a validation set or cross validation.</p>
<p>We will note that while choosing the number of trees <span class="math notranslate nohighlight">\(T\)</span> is necessary, it’s often not the most important thing to tune. Even though the curve looks a bit dramatic, note that the y-axis is clipped. In reality all of those test accuracies past about 250 trees are near the peak of ~86%. While they do decrease as <span class="math notranslate nohighlight">\(T\)</span> increases, it never really falls below 84% accuracy which is still fairly impressive for not doing anything too complicated in our tuning!</p>
<p>There are many hyperparameters we can choose to tune for our AdaBoost model, and are quite powerful if you spend time tuning the right parameters. Boosting models like AdaBoost and its variants are consistently the top-performing models in competitions on Kaggle.</p>
</section>
<section id="adaboost-properties">
<h3><span class="section-number">10.4.6. </span>AdaBoost Properties<a class="headerlink" href="#adaboost-properties" title="Permalink to this heading">#</a></h3>
<p>When thinking about AdaBoost, the following bullet points of important properties this model has in general is useful to keep in mind. Like always, these are just generalizations and it doesn’t mean you always should/shouldn’t use these models in a specific context. But they are useful rules of thumb to know.</p>
<ul class="simple">
<li><p>Boosting models are quite powerful and have achieved incredibly impressive performance on many real-world datasets. Typically, AdaBoost does better than Random Forests even with the same number of trees.</p></li>
<li><p>However, that doesn’t always make them the best bet as they tend to be more high-maintenance. While in AdaBoost the number of trees is only moderately important, other boosting variants like Gradient Boosting have a <em>ton</em> of hyperparameters that it is very sensitive to that need tuning.</p></li>
<li><p>Additionally, boosting methods are inherently more expensive to train since they need to be trained in sequence. Figuring out how to parallelize the training process for boosted methods is quite complicated, and nowhere near as simple as the procedure for training Random Forests in parallel.</p></li>
</ul>
</section>
</section>
<section id="recap-reflection">
<h2><span class="section-number">10.5. </span>Recap / Reflection<a class="headerlink" href="#recap-reflection" title="Permalink to this heading">#</a></h2>
<p>In this chapter, we started by defining how to handle features of various data types. We then explored the concept of ensemble methods, and explored specific types of ensembles. For bagging methods, we discussed the example of AdaBoost and for boosting methods, we discussed the example of AdaBoost. Importantly, we discussed the different tradeoffs they make in their designs and some common pointers for how to consider using one over the other.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./classification/ensembles"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../trees/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span><i class="fas fa-book fa-fw"></i> Decision Trees</p>
      </div>
    </a>
    <a class="right-next"
       href="../../deep_learning/intro/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span><i class="fas fa-book fa-fw"></i> Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aside-data-types">10.1. Aside: Data Types</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-data">10.1.1. Numeric Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-data">10.1.2. Categorical Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking">10.2. Stacking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-random-forests">10.3. Bagging - Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-algorithm">10.3.1. Random Forest Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-properties">10.3.2. Random Forest Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-code">10.3.3. Random Forest Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practicalities">10.3.4. Practicalities</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-features-m">10.3.4.1. Number of features <span class="math notranslate nohighlight">\(m\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-oob-error">10.3.4.2. Out of Bag (OOB) Error</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting-adaboost">10.4. Boosting - AdaBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-predictions">10.4.1. AdaBoost Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-training">10.4.2. AdaBoost Training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-from-weighted-data">10.4.2.1. Learning from Weighted Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-weights-hat-w-t">10.4.2.2. Model Weights <span class="math notranslate nohighlight">\(\hat{w}_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-dataset-weights-alpha-i-t-1">10.4.2.3. Computing Dataset Weights $\alpha_{i, t+1}</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">10.4.2.4. Putting it all Together</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-code">10.4.3. AdaBoost code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-walkthrough">10.4.4. AdaBoost Walkthrough</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">10.4.5. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-properties">10.4.6. AdaBoost Properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-reflection">10.5. Recap / Reflection</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hunter Schafer
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div style="float: right">
  <!-- 100% privacy friendly analytics -->
  <script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
  <noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>
  <a href="https://simpleanalytics.com/?utm_source=&utm_content=badge" referrerpolicy="origin" target="_blank"><picture><source srcset="https://simpleanalyticsbadges.com/?mode=dark" media="(prefers-color-scheme: dark)" /><img src="https://simpleanalyticsbadges.com/?mode=light" loading="lazy" referrerpolicy="no-referrer" crossorigin="anonymous" /></picture></a>
</div>

<div>
  <p>
    Have feedback or spotted a bug? Please make a <a href="https://github.com/animlbook/AnIML/issues">GitHub issue</a>
    or contact <a href="https://homes.cs.washington.edu/~hschafer/">Hunter Schafer</a>!
  </p>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>