

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>6. Logistic Regression &#8212; AnIML: Another Introduction to Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "animlbook/AnIML");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "preferred-color-scheme");
    script.setAttribute("label", "üí¨ Comments");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script src="../../_static/script.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmin": ["\\underset{#1}{\\operatorname{argmin}}", 1], "argmax": ["\\underset{#1}{\\operatorname{argmax}}", 1], "abs": ["\\lvert #1 \\rvert", 1], "indicator": ["\\mathbb{\\unicode{x1D7D9}}\\left\\{ #1 \\right\\}", 1], "norm": ["\\lVert #1 \\rVert", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'classification/logistic_regression/index';</script>
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. Bias and Fairness" href="../bias_fairness/index.html" />
    <link rel="prev" title="5. Classification Overview" href="../intro/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">This book is still under construction. We appreciate your patience as we get it completed. Feedback is welcome!</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro/index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro/index.html">
                    <i class="fas fa-hand-sparkles fa-fw"></i> Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../regression/linear_regression/index.html">1. <i class="fas fa-book fa-fw"></i> Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/assessing_performance/index.html">2. <i class="fas fa-book fa-fw"></i> Assessing Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/ridge/index.html">3. <i class="fas fa-book fa-fw"></i> Ridge Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/lasso/index.html">4. <i class="fas fa-book fa-fw"></i> Feature Selection and LASSO Regularization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/index.html">5. <i class="fas fa-book fa-fw"></i> Classification Overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. <i class="fas fa-book fa-fw"></i> Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bias_fairness/index.html">7. <i class="fas fa-book fa-fw"></i> Bias and Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../naive_bayes/index.html">8. <i class="fas fa-book fa-fw"></i> Na√Øve Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trees/index.html">9. <i class="fas fa-book fa-fw"></i> Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ensembles/index.html">10. <i class="fas fa-book fa-fw"></i> Ensemble Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../deep_learning/intro/index.html">11. <i class="fas fa-book fa-fw"></i> Neural Networks (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_learning/conv_nets/index.html">12. <i class="fas fa-book fa-fw"></i> Convolutional Neural Networks (coming soon)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Document Retrieval / Local Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/knn/index.html">13. <i class="fas fa-book fa-fw"></i> Introduction, Precision/Recall, k-Nearest Neighbors (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/kernels/index.html">14. <i class="fas fa-book fa-fw"></i> Kernel Methods (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/lsh/index.html">15. <i class="fas fa-book fa-fw"></i> Locality Sensitive Hashing (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/clustering/index.html">16. <i class="fas fa-book fa-fw"></i> Clustering &amp; k-means (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../document_retrieval/other_clustering/index.html">17. <i class="fas fa-book fa-fw"></i> Hierarchical Clustering (coming soon)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommender Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../recommender_systems/pca/index.html">18. <i class="fas fa-book fa-fw"></i> Dimensionality Reduction (coming soon)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recommender_systems/recommendation/index.html">19. <i class="fas fa-book fa-fw"></i> Recommender Systems</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/animlbook/AnIML/main?urlpath=lab/tree/book_source/source/classification/logistic_regression/index.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/animlbook/AnIML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/animlbook/AnIML/edit/main/book_source/source/classification/logistic_regression/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/animlbook/AnIML/issues/new?title=Issue%20on%20page%20%2Fclassification/logistic_regression/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/classification/logistic_regression/index.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/classification/logistic_regression/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1><i class="fas fa-book fa-fw"></i> Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-classification-error">6.1. Minimizing Classification Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-predictions">6.2. Probability Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">6.3. Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scores-to-probabilities-a-sigmoid-approach">6.3.1. Scores to Probabilities: A Sigmoid Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-model">6.3.2. Logistic Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-code">6.3.3. Logistic Regression Code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">6.4. Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-ascent">6.5. Gradient Ascent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-size">6.5.1. Step Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">6.5.2. Grid Search</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-and-overfitting">6.6. Logistic Regression and Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-and-regularization">6.7. Logistic Regression and Regularization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">6.8. Recap</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="i-class-fas-fa-book-fa-fw-i-logistic-regression">
<h1><span class="section-number">6. </span><i class="fas fa-book fa-fw"></i> Logistic Regression<a class="headerlink" href="#i-class-fas-fa-book-fa-fw-i-logistic-regression" title="Permalink to this heading">#</a></h1>
<p>In our last chapter on <a class="reference internal" href="../intro/index.html"><span class="doc"> Classification Overview</span></a> we introduced the terminology of the classification, some first attempts at defining models for this scenario, and evaluation metrics for evaluating these models.</p>
<p>Our most successful model discussed so far was the Simple Linear Classifier. Recall that in the context of sentiment analysis, the goal of our linear classifier was to predict whether or not a sentence was positive or negative sentiment. We defined the predicted score of a sentence <span class="math notranslate nohighlight">\(x_i\)</span> as <span class="math notranslate nohighlight">\(\hat{Score}(x_i) = \hat{w}^T h(x_i)\)</span>. If this score was positive, we predicted positive sentiment and if it was negative predicted negative sentiment.</p>
<p>The only detail we left out was how to learn its coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span>. With the story we saw so far in our regression setting, let‚Äôs follow a similar path to discuss how to learn our parameters by using some optimization algorithm over some quality metric.</p>
<section id="minimizing-classification-error">
<h2><span class="section-number">6.1. </span>Minimizing Classification Error<a class="headerlink" href="#minimizing-classification-error" title="Permalink to this heading">#</a></h2>
<p>Intuitively, we would probably want to find the coefficients for our linear model that minimize some notion of classification error. Mathematically, we could write our goal is to minimize this quality metric:</p>
<div class="math notranslate nohighlight">
\[\hat{w} = \argmin{w} \frac{1}{n}\sum_{i=1}^n \indicator{y_i \neq \hat{y}_i}\]</div>
<p>Just like with our regression setting, it is infeasible to try all possible settings of <span class="math notranslate nohighlight">\(w\)</span>, so instead we could hopefully use some optimization algorithm such as gradient descent to ‚Äúroll down the hill‚Äù and find the best setting of our parameters. Unfortunately, that won‚Äôt work here.</p>
<p>Recall that there were actually some pretty important assumptions about the function we wanted to optimize.</p>
<ul class="simple">
<li><p>The function we are optimizing has to be ‚Äúbowl shaped‚Äù or <em>convex</em> so that the local optima is guaranteed to be the global optima (otherwise we get stuck at a sub-optimal valley)</p></li>
<li><p>While we didn‚Äôt discuss this in detail, we also need some basic qualities of our function to allow optimization such that the function is continuous and differentiable, so that it even makes sense to discuss computing the slope/gradient to ‚Äúroll down the hill‚Äù</p></li>
</ul>
<p>It turns out that classification error, has none of these desirable properties! Consider the following plot that shows the classification error as we vary a single parameter <span class="math notranslate nohighlight">\(w_1\)</span>. Depending on how many errors that line makes for that setting of <span class="math notranslate nohighlight">\(w_1\)</span>, its classification error will be lower or higher. The problem though is the errors don‚Äôt change <em>continuously</em>. The error suddenly jumps up/down once the decision boundary crosses one of the training points. This causes a huge problem for gradient descent methods since there is no notion of smoothly ‚Äúrolling down the hill‚Äù here. Every starting point looks like a stopping point because slightly changing the slope <span class="math notranslate nohighlight">\(w_1\)</span> doesn‚Äôt change the classification error!</p>
<figure class="align-center">
<img alt="A graph depicting that the classification error error metric does not have any of the nice properties we want for gradient descent (explained in last paragraph)" src="../../_images/loss_function.png" />
</figure>
<p>So even though minimizing classification error seems like what we want to do, it‚Äôs actually possible in practice. In order to fix this, we will have to slightly change our problem statement to make our problem one that we can optimize.</p>
</section>
<section id="probability-predictions">
<h2><span class="section-number">6.2. </span>Probability Predictions<a class="headerlink" href="#probability-predictions" title="Permalink to this heading">#</a></h2>
<p>Instead of only caring about the discrete outputs <span class="math notranslate nohighlight">\(\hat{y}_i = +1\)</span> or <span class="math notranslate nohighlight">\(\hat{y}_i = -1\)</span>, we will move our discussion to the world of probabilities. In other words, we will ask the question ‚ÄúGiven the sentence ‚ÄòThe sushi and everything else were awesome‚Äô, what is the probability this sentence is a positive review?‚Äù To notate this mathematically, we are interested in the quantity</p>
<div class="math notranslate nohighlight">
\[P(y = +1 | x = \text{&quot;The Sushi and everything else were awesome&quot;})\]</div>
<p>For a sentence as clearly positive as this one, we might expect this probability to be quite high, maybe more than 0.99!</p>
<p>But for more ambiguous sentences, we might not be certain either way and the probability of a positive review is lower. For example we might consider another example.</p>
<div class="math notranslate nohighlight">
\[P(y = +1 | x = \text{&quot;The sushi was alright, the service was OK&quot;}) \approx 0.5\]</div>
<p>Note that we could also ask the question of what is the probability of a negative review <span class="math notranslate nohighlight">\(P(y = -1 | x)\)</span>. But since there are only two outcomes, there is 1:1 relationship between positive/negative review probabilities as shown in the equation below. This means we can always switch which probabilities we are discussing by subtracting them from 1.</p>
<div class="math notranslate nohighlight">
\[P(y = +1 | x) + P(y = -1 | x) = 1\]</div>
<p>This leads us to a notion of a classifier that utilizes these probabilities.</p>
<div class="proof algorithm admonition" id="probability-classifier">
<p class="admonition-title"><span class="caption-number">Algorithm 6.1 </span> (Probability-based Classifier)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> A sentence <span class="math notranslate nohighlight">\(x\)</span>, and some model to predict <span class="math notranslate nohighlight">\(\hat{P}(y = +1 | x)\)</span></p>
<p><strong>Output</strong> A class <span class="math notranslate nohighlight">\(\hat{y} = +1\)</span> or <span class="math notranslate nohighlight">\(\hat{y} = =1\)</span></p>
<ol class="arabic simple">
<li><p>Estimate probability of positive review <span class="math notranslate nohighlight">\(p \gets \hat{P}(y = +1 | x)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = +1\)</span></p></li>
</ol>
</li>
<li><p>else</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = -1\)</span></p></li>
</ol>
</li>
</ol>
</section>
</div><p>One benefit of this approach is by modifying this procedure to also report <span class="math notranslate nohighlight">\(\hat{P}(y = +1|x)\)</span>, it makes our model a bit more interpretable because we get a notion of confidence in the model‚Äôs predictions.</p>
<p>While this is all well and good in theory, we still haven‚Äôt answered how to exactly learn a model to match probabilities instead of just matching labels.</p>
</section>
<section id="logistic-regression">
<h2><span class="section-number">6.3. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h2>
<section id="scores-to-probabilities-a-sigmoid-approach">
<h3><span class="section-number">6.3.1. </span>Scores to Probabilities: A Sigmoid Approach<a class="headerlink" href="#scores-to-probabilities-a-sigmoid-approach" title="Permalink to this heading">#</a></h3>
<p>What if the scores we were trying to learn for our Simple Linear Classifier could actually help us define some notion of probability? In theory the scores we were learning encoded some notion of a sliding scale between positive/negative. The scores could range from extremely negative numbers to extremely positive numbers. What if we could just change the units of the Scores so they would be ranged between the values of probabilities of 0s to 1s.</p>
<p>What we would need is some transformation from the units of scores (which could range from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>) to the units of probabilities (which range from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>). Our interpretation of the scores are as follows:</p>
<ul class="simple">
<li><p>When the score is very negative (near <span class="math notranslate nohighlight">\(-\infty\)</span>), we are very sure the predicted label should be <span class="math notranslate nohighlight">\(\hat{y} = -1\)</span> and therefore the <span class="math notranslate nohighlight">\(\hat{P}(y = +1 | x ) \approx 0\)</span>.</p></li>
<li><p>When the score is very positive (near <span class="math notranslate nohighlight">\(\infty\)</span>), we are very sure the predicted label should be <span class="math notranslate nohighlight">\(\hat{y} = +1\)</span> and therefore the <span class="math notranslate nohighlight">\(\hat{P})y = +1 | x) \approx 1\)</span>.</p></li>
<li><p>When the score is near zero, we aren‚Äôt very sure either way and we could say <span class="math notranslate nohighlight">\(\hat{P}(y = +1 | x) \approx 0.5\)</span>.</p></li>
</ul>
<p>There are many functions that can compress the range of a number from <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span> to <span class="math notranslate nohighlight">\((0, 1)\)</span>, but the one we will discuss in detail is the <strong>sigmoid</strong> or the <strong>logistic function</strong>.</p>
<p>The sigmoid is defined as follows.</p>
<div class="math notranslate nohighlight">
\[sigmoid(z) = \frac{1}{1 + e^{-z}}\]</div>
<p>In our case, the inputs of the function will be the scores themselves. So we will try as our probability model</p>
<div class="math notranslate nohighlight">
\[P(y = +1 | x) = sigmoid(Score(x)) = \frac{1}{1 + e^{-Score(x)}}\]</div>
<p>Note that this function has all of the properties we want to convert from scores to probabilities. We show some examples of this functions outputs in the table below, and in the figure below that, plot out its shape.</p>
<table class="table" id="logistic-outputs">
<caption><span class="caption-number">Table 6.1 </span><span class="caption-text">Example outputs for Logistic Function</span><a class="headerlink" href="#logistic-outputs" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(Score(x)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(sigmoid(Score(x))\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(-\infty\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{--\infty}} = \frac{1}{1 + e^\infty} = \frac{1}{1 + \text{large number}} \approx 0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(-2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{--2}} \approx 0.12\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-0}} = \frac{1}{1 + 1} = 0.5\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-2}} \approx 0.88\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(-\infty\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-\infty}} = \frac{1}{1 + 1/e^\infty} = \frac{1}{1 + \text{number near 0}} \approx 1\)</span></p></td>
</tr>
</tbody>
</table>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/logistic.png"><img alt="Graph of the s-curve shape of logistic regression" src="../../_images/logistic.png" style="width: 60%;" /></a>
</figure>
</section>
<section id="logistic-regression-model">
<h3><span class="section-number">6.3.2. </span>Logistic Regression Model<a class="headerlink" href="#logistic-regression-model" title="Permalink to this heading">#</a></h3>
<p>So with this idea of how we can transform scores to probabilities, we can now arrive at our important <strong>Logistic Regression</strong> model. Recall that the <span class="math notranslate nohighlight">\(\hat{Score}(x)\)</span> is determined by our learned coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span>, so it‚Äôs appropriate to notate that our probability are a function of which coefficients we choose <span class="math notranslate nohighlight">\(w\)</span>.</p>
<div class="math notranslate nohighlight">
\[P(y_i = +1 | x_i, w) = sigmoid(Score_w(x_i)) = \frac{1}{1 + e^{-w^Th(x_i)}}\]</div>
<p>We can then use this in our <strong>Logistic Regression</strong> model</p>
<div class="proof algorithm admonition" id="logistic-classifier">
<p class="admonition-title"><span class="caption-number">Algorithm 6.2 </span> (Logistic Regression Classifier)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> A sentence <span class="math notranslate nohighlight">\(x\)</span>, and our learned coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span></p>
<p><strong>Output</strong> A class <span class="math notranslate nohighlight">\(\hat{y} = +1\)</span> or <span class="math notranslate nohighlight">\(\hat{y} = =1\)</span></p>
<ol class="arabic simple">
<li><p>Estimate probability of positive review <span class="math notranslate nohighlight">\(p \gets \hat{P}(y = +1 | x, \hat{w}) = sigmoid(\hat{w}^Th(x))\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = +1\)</span></p></li>
</ol>
</li>
<li><p>else</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = -1\)</span></p></li>
</ol>
</li>
</ol>
</section>
</div><p>It might help to visualize a bit what is going on here. Recall before that our untransformed <span class="math notranslate nohighlight">\(Score\)</span> function was a linear function in terms of the inputs. Now, we are transforming all of the score outputs to now follow this S-curve of the logistic where high scores are near 1 and low scores are near zero. We can still imagine the same decision boundary, but now that decision boundary is when the probability is near 0.5 (score is near 0).</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Bunch of imports to get 3D plots</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d.art3d</span> <span class="kn">import</span> <span class="n">Poly3DCollection</span>

<span class="c1"># Make our color scheme match our drawings from earlier</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Set up global plots</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">LinearSegmentedColormap</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="s1">&#39;custom&#39;</span><span class="p">,</span>
    <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;#D89F39&#39;</span><span class="p">),</span>
     <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;#FFFFFF&#39;</span><span class="p">),</span>
     <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;#3A81BA&#39;</span><span class="p">)],</span> <span class="n">N</span><span class="o">=</span><span class="mi">126</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">6.0</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_score_function</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
    <span class="c1"># Set up function and evalautes</span>

    <span class="k">def</span> <span class="nf">z_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">z_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="c1"># Make 3d plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">15</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;#awesome&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;#awful&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">230</span><span class="p">)</span>

    <span class="c1"># Draw decision boundary</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Linear Scores&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_logistic_scores</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">z_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
      <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">)))</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">z_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;#awesome&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;#awful&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;sigmoid(score)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">230</span><span class="p">)</span>

    <span class="c1"># Draw shaded region for positive label</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">Poly3DCollection</span><span class="p">([[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="o">/</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)]])</span>
    <span class="n">pos</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">pos</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;#3A81BA&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_collection</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>

    <span class="c1"># Draw shaded region for negative label</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">Poly3DCollection</span><span class="p">([[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="o">/</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)]])</span>
    <span class="n">neg</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">neg</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;#D89F39&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_collection</span><span class="p">(</span><span class="n">neg</span><span class="p">)</span>

    <span class="c1"># Draw decision boundary</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Logistic Regression&quot;</span><span class="p">)</span>

<span class="n">plot_score_function</span><span class="p">(</span><span class="n">ax1</span><span class="p">)</span>
<span class="n">plot_logistic_scores</span><span class="p">(</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../_images/c873bc97cc2008c25f1bc077adfd179ec8bdc367ff1ab1710c3f64301f134efe.png"><img alt="../../_images/c873bc97cc2008c25f1bc077adfd179ec8bdc367ff1ab1710c3f64301f134efe.png" class="align-center" src="../../_images/c873bc97cc2008c25f1bc077adfd179ec8bdc367ff1ab1710c3f64301f134efe.png" style="width: 100%;" /></a>
</div>
</div>
<p>We‚Äôll come back to how to learn our coefficients in the next section, but before then, we want to show a bit of a practical example for how to use the scikit-learn library‚Äôs implementation for Logistic Regression.</p>
</section>
<section id="logistic-regression-code">
<h3><span class="section-number">6.3.3. </span>Logistic Regression Code<a class="headerlink" href="#logistic-regression-code" title="Permalink to this heading">#</a></h3>
<p>Despite the complicated math in formulating the model, using a Logistic Regression model for classification in scikit-learn is just as easy as our normal Linear Regression model. We use the same <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> methods like before! There is an added method <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> to display what the probability of the predicted labels are.</p>
<p>The dataset we use is a cleaned version of an online dataset of product reviews from Amazon. While a huge part of a data scientist‚Äôs task is pre-processing the data, we omit these steps to focus on the learning task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># For display</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;cleaned_products.csv&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">&quot;**Data**&quot;</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Note we would also use a validation set if we wanted to</span>
<span class="c1"># choose a hyperparameter, but omit in this example</span>
<span class="c1"># random_state is to get consistent results</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Transform the data to the bag of words model</span>
<span class="c1"># Important: Use the training data words to make bag-of-words counts for test</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">])</span>  <span class="c1"># Only count words in train set</span>
<span class="n">train_counts</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">])</span>
<span class="n">test_counts</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">])</span>

<span class="c1"># Train a model (random_state to get consistent results)</span>
<span class="n">sentiment_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sentiment_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_counts</span><span class="p">,</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">])</span>

<span class="c1"># Make predictions on data</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="n">sentiment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_counts</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">&quot;**Train Predictions** (first few examples)&quot;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions&quot;</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Actual     &quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;sentiment&quot;</span><span class="p">][:</span><span class="mi">5</span><span class="p">]))</span>

<span class="c1"># Make probability predictions</span>
<span class="c1"># Returns a 2D array of [Negative Prob, Positive Prob]</span>
<span class="n">train_prob_predictions</span> <span class="o">=</span> <span class="n">sentiment_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">train_counts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">,</span> <span class="n">train_prob_predictions</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Evaluate accuracy of model</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">&quot;**Model Accuracy**&quot;</span><span class="p">))</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">sentiment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_counts</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train Accuracy&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;sentiment&quot;</span><span class="p">],</span> <span class="n">train_predictions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  Accuracy&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;sentiment&quot;</span><span class="p">],</span> <span class="n">test_predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p><strong>Data</strong></p>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>review</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1</td>
      <td>I should have stayed with Idahoan brand Poor B...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>As the pasta cooked I read the box to see what...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>I really like this cereal The flavor is slight...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1</td>
      <td>The label on the bowl says 35 grams is in the ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1</td>
      <td>Doesnt taste like ginger  Thought it would sav...</td>
    </tr>
  </tbody>
</table>
</div></div><p><strong>Train Predictions</strong> (first few examples)</p>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions [ 1 -1 -1  1  1]
Actual      [1, -1, -1, 1, 1]
Probability [0.89974777 0.14569628 0.16639926 0.95383164 0.98689545]
</pre></div>
</div>
<p><strong>Model Accuracy</strong></p>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train Accuracy 1.0
Test  Accuracy 0.7415730337078652
</pre></div>
</div>
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Important: Data Transformations</p>
<p>Note in the example above, we had to use the same process to convert our data in the bag of words model from our train set to our test set. This is the same idea as why we had to normalize our data with the training statistics! Any data transformations you do have to be done entirely from the training set and no other outside data.</p>
</div>
<p>Now that we have seen how to use Logistic Regression in practice, let‚Äôs discuss the procedure for learning our coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span> that result in our probability predictions.</p>
</section>
</section>
<section id="maximum-likelihood-estimation-mle">
<h2><span class="section-number">6.4. </span>Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Permalink to this heading">#</a></h2>
<p>While this switch to probability is nice, it is not the only change we have to make in order to learn our coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span>. In particular, it‚Äôs still not possible to minimize our classification error quality metric, even in the setting with probabilities. You might think of trying to move this back to a regression setting with using the MSE quality metrics with probabilities, but it turns out this also doesn‚Äôt work in practice since the assumptions we made for our regression setting don‚Äôt quite apply here in this bounded probability setting.</p>
<p>Instead, we have to change the question we care to optimize a bit. Instead of minimizing some notion of error with our concepts of probabilities, what if we use those probabilities as our measure of fit. In particular, we want our probabilities to ‚Äúline up‚Äù with the true labels of the data. In other words, when the true label of an example is <span class="math notranslate nohighlight">\(y_i = +1\)</span>, we want <span class="math notranslate nohighlight">\(\hat{P}(y_i = +1 | x_i, \hat{w})\)</span> to be high. On the other hand, when <span class="math notranslate nohighlight">\(y_i = -1\)</span> we want <span class="math notranslate nohighlight">\(\hat{P}(y_i = +1 | x_i, \hat{w})\)</span> to be low (conversely <span class="math notranslate nohighlight">\(\hat{P}(y_i = -1 | x_i, \hat{w})\)</span> to be high). If we can find the coefficients <span class="math notranslate nohighlight">\(\hat{w}\)</span> that make these probabilities ‚Äúline up‚Äù, then our hope is that those coefficients are good coefficients. Note that this sounds a bit like minimizing error, and it is kind of like that, but is slightly different since we are just trying to find coefficients that are likely to explain the data we saw.</p>
<p>This brings us to to the concept of <strong>likelihood</strong> (also called <strong>likelihood functions</strong>). The concept of a likelihood is exactly the concept we explained above. The likelihood measures, given a setting of coefficients we interested in exploring <span class="math notranslate nohighlight">\(w\)</span>, how likely that setting of the coefficients explains the training data we have.</p>
<p>Consider just a single training point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>. We can define the likelihood <span class="math notranslate nohighlight">\(\ell_i(w)\)</span> to explain how likely seeing label <span class="math notranslate nohighlight">\(y_i\)</span> for input <span class="math notranslate nohighlight">\(x_i\)</span> when using coefficients <span class="math notranslate nohighlight">\(w\)</span>. Notation-wise, we use the following to address the likelihood for a single data point to simplify the if/else condition if we care about the probability of a positive label or probability of a negative label.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \ell_i(w) = \hat{P}(y_i | x_i, w) = \begin{cases}
        \hat{P}(y_i = +1 | x_i, w) &amp; \text{if } y_i = +1\\
        \hat{P}(y_i = -1 | x_i, w) &amp; \text{if } y_i = -1
  \end{cases}
\end{split}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>1. Note that because <span class="math notranslate nohighlight">\(P(y = +1|x) + P(y = -1|x) = 1\)</span>, we can derive the formula for the probability of the negative label by <span class="math notranslate nohighlight">\(P(y=-1|x) = 1 - P(y=+1|x)\)</span>.</p>
</aside>
<p>Recall the probabilities are determined by our sigmoid function, so more explicitly, we have the following<sup>1</sup>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \ell_i(w) = \hat{P}(y_i | x_i, w) = \begin{cases}
        \frac{1}{1 + e^{-w^Th(x_i)}} &amp; \text{if } y_i = +1\\
        \frac{e^{-w^Th(x_i)}}{1 + e^{-w^Th(x_i)}}&amp; \text{if } y_i = -1
  \end{cases}
\end{split}\]</div>
<p>The intuition for this formula matches our description above. If the label is a positive label, the likelihood for our coefficients is higher if the probability of a positive label it predicts is larger. Similarly, if the label is a negative label, the likelihood is higher is the probability of a negative label is higher.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>2. üìù <em>Notation</em>: The <span class="math notranslate nohighlight">\(\Pi\)</span> notation is a cumulative product just like <span class="math notranslate nohighlight">\(\sum\)</span> is a cumulative sum.</p>
</aside>
<p>But we don‚Äôt care about just one data point, so instead we define the likelihood for our entire training dataset <span class="math notranslate nohighlight">\(\ell(w)\)</span> as the following<sup>2</sup>. Recall this definition has that hidden if-statement from our definition above to check the probability that matches the label of the training point.</p>
<div class="math notranslate nohighlight">
\[\ell(w) = \hat{P}(y_1|x_1, w) \cdot \hat{P}(y_2|x_2, w)\cdot ... \cdot \hat{P}(y_n|x_n, w) = \Pi_{i=1}^n \hat{P}(y_i | x_i, w)\]</div>
<p>The intuition for this measure is that the the likelihood of our coefficients <span class="math notranslate nohighlight">\(w\)</span> is high when the probabilities ‚Äúline up‚Äù with the true labels. For example, consider the following graph of a small dataset and two candidates settings of coefficients <span class="math notranslate nohighlight">\(w^{(1)}\)</span> and <span class="math notranslate nohighlight">\(w^{(2)}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">positive_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">negative_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Find min/max of data above (to be flexible)</span>
<span class="n">min_x</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">max_x</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">min_y</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="n">max_y</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Make lines for classifier</span>

<span class="k">def</span> <span class="nf">boundary1</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.45</span> <span class="o">*</span> <span class="n">xs</span> <span class="o">+</span> <span class="mf">0.85</span>

<span class="k">def</span> <span class="nf">boundary2</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">9.5</span> <span class="o">-</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="n">xs</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">max_x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">line_1</span> <span class="o">=</span> <span class="n">boundary1</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">line_2</span> <span class="o">=</span> <span class="n">boundary2</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>

<span class="c1"># Make plots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">positive_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;P&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Positive Example&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">negative_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Negative Example&quot;</span><span class="p">)</span>

<span class="c1"># Plot boundaries</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">line_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$w^{(1)}$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">line_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$w^{(2)}$&quot;</span><span class="p">)</span>

<span class="c1"># Plot indicators for positive direction (somewhat hardcoded for simplicty)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">boundary1</span><span class="p">(</span><span class="mf">0.8</span><span class="p">),</span> <span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.07</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Dir. Positive&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">boundary2</span><span class="p">(</span><span class="mf">1.5</span><span class="p">),</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.07</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Dir. Positive&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>

<span class="c1"># Customize plot</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="n">min_x</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">max_x</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="n">min_y</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">max_y</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;# awesome&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;# awful&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../_images/3bb7cdc9b37ef51d4bb962c8f9ecbfd4ea49c964c9110e27c289f0a5c7f4a604.png"><img alt="../../_images/3bb7cdc9b37ef51d4bb962c8f9ecbfd4ea49c964c9110e27c289f0a5c7f4a604.png" class="align-center" src="../../_images/3bb7cdc9b37ef51d4bb962c8f9ecbfd4ea49c964c9110e27c289f0a5c7f4a604.png" style="width: 75%;" /></a>
</div>
</div>
<p>If you went through and did the math, the likelihood of <span class="math notranslate nohighlight">\(w^{(1)}\)</span> would be higher than <span class="math notranslate nohighlight">\(w^{(2)}\)</span> because its probability outputs are more ‚Äúlined up‚Äù with the true labels. The second set of coefficients makes low-probability predictions of the correct labels on some points since they are on the opposite side of the decision boundary (i.e., more likely to be the opposite label). So the nice thing is this likelihood has some intuition like classification error, but it is a smoother notion since we are rewarding settings of <span class="math notranslate nohighlight">\(w\)</span> that have probabilities that are closer to the true labels.</p>
<p>So with all of this discussion in mind, you might start to realize that we care about <em>maximizing</em> this likelihood <span class="math notranslate nohighlight">\(\ell(w)\)</span> as a way of finding the ‚Äúbest‚Äù coefficients. So mathematically, our optimization problem would the following <strong>maximum likelihood estimator</strong>.</p>
<div class="math notranslate nohighlight">
\[\hat{w} = \argmax{w} \ell(w) = \argmax{w} \Pi_{i=1}^n \hat{P}(y_i|x_i, w)\]</div>
<p>While this works in theory, in practice we tend to not optimize the likelihood directly. Recall that all of the probabilities are between 0 and 1. So multiplying a bunch of numbers between 0 and 1 over a dataset with, say, 10000 examples would become vanishingly small; so small that our computer may run into errors of precision. Instead we commonly will optimize the <strong>log-likelihood</strong> because it is computationally more convenient. Note that the coefficients that maximize the log-likelihood will also maximize the likelihood, since <span class="math notranslate nohighlight">\(\log\)</span> is a monotonically increasing function. Also note that the property of the logarithm <span class="math notranslate nohighlight">\(\log(ab) = \log(a) + \log(b)\)</span> turns our problem into one of sums instead of products.</p>
<div class="math notranslate nohighlight">
\[\hat{w} = \argmax{w} \log(\ell(w)) = \argmax{w} \sum_{i=1}^n \log\left(\hat{P}(y_i|x_i, w)\right)\]</div>
<p>It turns out that this quality metric has all of the properties we want in our quality metric to use a gradient-based algorithm for finding the optimum. Before discussing this, we want to show one last way of formulating this log-likelihood quality metric that you might see in practice.</p>
<p>Remember that our quality metric has a hidden if-statement in there with our notation of <span class="math notranslate nohighlight">\(\hat{P}(y_i|x_i, w)\)</span>. That means you really have to compute the probabilities for positive/negative examples differently, so it sometimes helps to be explicit and write out the formula for all of the computations we need. So by separating or examples into positive and negative ones, and plugging in our formulas for our probabilities, we have a finalized and fully-detailed description of our quality metric.</p>
<div class="math notranslate nohighlight">
\[\hat{w} = \argmax{w} \sum_{i=1; y_i = +1}^n \log\left(\frac{1}{1 + e^{-w^Th(x_i)}}\right) + \sum_{i=1; y_i = -1}^n \log\left(\frac{e^{-w^Th(x_i)}}{1 + e^{-w^Th(x_i)}}\right)\]</div>
</section>
<section id="gradient-ascent">
<h2><span class="section-number">6.5. </span>Gradient Ascent<a class="headerlink" href="#gradient-ascent" title="Permalink to this heading">#</a></h2>
<p>As we alluded to, we care about <em>maximizing</em> this quality metric, so an algorithm like gradient descent won‚Äôt quite work here. It turns out though, that we have a completely analogous algorithm called <strong>gradient ascent</strong> that ‚Äúwalks up that hill‚Äù instead of ‚Äúrolling down the hill‚Äù. The ideas an computations for the algorithm are exactly the same, except we follow the slope/gradient of the function since that points in the direction of steepest ascent.</p>
<div class="proof algorithm admonition" id="grad-ascent">
<p class="admonition-title"><span class="caption-number">Algorithm 6.3 </span> (Gradient Ascent)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Start at some (random) point <span class="math notranslate nohighlight">\(w^{(0)}\)</span> at <span class="math notranslate nohighlight">\(t = 0\)</span></p></li>
<li><p>While we haven‚Äôt converged:</p>
<ol class="arabic simple">
<li><p>Compute gradient at current point (direction of ascent) <span class="math notranslate nohighlight">\(d \gets \nabla \log\left(P(y_i|x_i, w)\right)\)</span></p></li>
<li><p>Update point <span class="math notranslate nohighlight">\(w^{(t + 1)} \gets w^{(t)} + \eta d\)</span></p></li>
<li><p>Update time <span class="math notranslate nohighlight">\(t \gets t + 1\)</span></p></li>
</ol>
</li>
<li><p>Output current <span class="math notranslate nohighlight">\(w^{(t)}\)</span></p></li>
</ol>
</section>
</div><p>Again, we do an addition instead of a subtraction from <a class="reference internal" href="../../regression/linear_regression/index.html#grad-descent">Algorithm 1.1</a> since the slope/gradient points in the direction of increase.</p>
<p>Note that there are necessary conditions for gradient ascent, to guarantee it converges to a meaningful optimum. Just like with gradient descent, we need the function to be continuous and differentiable. Instead of needing the function to be ‚Äúbowl like‚Äù (<em>convex</em>) for gradient descent, we need the function to be ‚Äúhill like‚Äù (<em>concave</em>) in gradient ascent.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/likelihood.png"><img alt="A visualization of the likelihood &quot;hill&quot;" src="../../_images/likelihood.png" style="width: 75%;" /></a>
</figure>
<p>So that‚Äôs great, we basically understand this algorithm now because of its mirror relationship to gradient descent. Now that we have introduced both of these optimization algorithms, let‚Äôs discuss in detail what this extra hyperparamter <span class="math notranslate nohighlight">\(\eta\)</span> or the <strong>learning rate</strong>  or the <strong>step size</strong> affects these gradient algorithms.</p>
<section id="step-size">
<h3><span class="section-number">6.5.1. </span>Step Size<a class="headerlink" href="#step-size" title="Permalink to this heading">#</a></h3>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>While technically we are discussing gradient ascent here, everything we discuss in this section applies to gradient descent as well.</p>
</div>
<p>At a high level, <span class="math notranslate nohighlight">\(\eta\)</span> (pronounce ‚Äúeta‚Äù) is the step-size that is responsible for how far we move at each step of our gradient ascent algorithm. If <span class="math notranslate nohighlight">\(\eta\)</span> is larger, we take a larger step at each iteration of the algorithm, if <span class="math notranslate nohighlight">\(\eta\)</span> is smaller, we take smaller steps.</p>
<!-- TODO Animations would be really good in this section -->
<p>We can see the effect of how <span class="math notranslate nohighlight">\(\eta\)</span> impacts the quality of the model we learn, by looking at how our quality metric improves as we let the algorithm run for longer and longer iterations.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/grad_ascent_just_right.png"><img alt="Plot of quality metric value as" src="../../_images/grad_ascent_just_right.png" style="width: 100%;" /></a>
</figure>
<p>As you can imagine, if we make <span class="math notranslate nohighlight">\(\eta\)</span> smaller, we will still steadily climb up the hill but at a much slower pace. This will still be guaranteed to find the top of the hill, but might require a huge number of iterations to get there if <span class="math notranslate nohighlight">\(\eta\)</span> is too small.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/grad_ascent_small.png"><img alt="With a smaller eta, the rate we converge to the optimum is much slower" src="../../_images/grad_ascent_small.png" style="width: 100%;" /></a>
</figure>
<p>On the other hand, if we make <span class="math notranslate nohighlight">\(\eta\)</span> larger we can hopefully reach the top of the hill faster. The plot below shows a run of gradient ascent with a larger <span class="math notranslate nohighlight">\(\eta\)</span> and we do see it reaches a higher value of the quality metric sooner. Note the spiky behavior at the beginning of the training. This happens because our step size is so large it hops to ‚Äúthe other side of the hill.‚Äù This can potentially cause drops in our quality metric objective if we stepped a little too far and ended up further down than where we started. In this case <span class="math notranslate nohighlight">\(\eta\)</span> was large enough to cause a bit of jumping around, but we eventually converge to the optimum.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/grad_ascent_larger.png"><img alt="With a larger eta, the rate we converge to the optimum is much faster with some &quot;turbulance&quot;" src="../../_images/grad_ascent_larger.png" style="width: 100%;" /></a>
</figure>
<p>But if we make <span class="math notranslate nohighlight">\(\eta\)</span> much larger, this ‚Äújumping to the other side of the hill‚Äù can get so extreme that every time we step, we get <em>further away</em> from the optimum! In this extreme case, we will never converge because we are always jumping way too far away. This problem is exacerbated by the fact that the size of the step is not only controlled by <span class="math notranslate nohighlight">\(\eta\)</span>, but also the magnitude of the slope/gradient at that point. With concave functions, the slopes are generally larger the farther you are away from the optimum, so our stepping further away causes an even larger jump on the next step, causing us to be even further away! Note the other choices of <span class="math notranslate nohighlight">\(\eta\)</span> are shown in the plot below, and you can‚Äôt even tell the difference between how they converge to the optimum compared to just <em>how bad</em> this large setting of <span class="math notranslate nohighlight">\(\eta\)</span> is!</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/grad_ascent_too_large.png"><img alt="With a much larger eta, we never converge" src="../../_images/grad_ascent_too_large.png" style="width: 100%;" /></a>
</figure>
<p>How do you choose the right step-size then? Just like most of our other hyperparameters, <em>you just have to guess and check a lot of options to see which one does best on some validation set</em>. This won‚Äôt make complete sense why we need a validation set to figure out a detail of our optimization algorithm that seemingly has no effect on the model itself, we‚Äôll see later on that certain model types tend to overfit as we let them train longer and longer on our training set.</p>
<p>A good rule of thumb for trying out learning rates is to try values that are <strong>exponentially spaced</strong> to get a wider range of values you are trying. So instead of trying out <span class="math notranslate nohighlight">\(\eta = 1, 2, 3, ...\)</span>, try <span class="math notranslate nohighlight">\(\eta = 10^{-5}, 10^{-4}, ..., 1, 10, 100, ...\)</span>. That gives you a ballpark of what is the right order of magnitude for your task, and you can explore more values near the right order of magnitude.</p>
<p>A more advanced approach is to use a <strong>decreasing step-size</strong> as the gradient ascent algorithm runs. We tend to find this divergence from the optimum to happen as we approach the optimal point with a step size that is too large. So one clever fix to this is to decrease the step-size as the algorithm runs to hopefully prevent this. We just have to choose some starter step size <span class="math notranslate nohighlight">\(\eta_0\)</span> and then at each iteration <span class="math notranslate nohighlight">\(t\)</span>, use a step-size like the following (or some other transformation that makes the value smaller over time).</p>
<div class="math notranslate nohighlight">
\[\eta_t = \frac{\eta_0}{t}\]</div>
</section>
<section id="grid-search">
<h3><span class="section-number">6.5.2. </span>Grid Search<a class="headerlink" href="#grid-search" title="Permalink to this heading">#</a></h3>
<p>As we highlighted that for most hyperparameters, the most common algorithms are ones that just try all of the options. What about when we want to choose settings for multiple hyperparameters? For example, what if we want to choose the best learning rate <span class="math notranslate nohighlight">\(\eta\)</span> and the best Ridge regularization constant <span class="math notranslate nohighlight">\(\lambda\)</span>?</p>
<p>Unfortunately, it is not sufficient to find the the best setting of <span class="math notranslate nohighlight">\(\eta\)</span> using one cross validation experiment, and then in another cross validation experiment, find the best option for <span class="math notranslate nohighlight">\(\lambda\)</span>. That procedure is only guaranteed to work if you know the settings of <span class="math notranslate nohighlight">\(\eta\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> have independent on the effects of the model. Unfortunately, it is quite hard to make that argument for most hyperparamters. Most hyperparameters have complex effects on our model‚Äôs behaviors, and it‚Äôs possible there is some complex relationship between how our hyperparamters effect each other even if we can‚Äôt find some intuitive relationship between them.</p>
<p>What that means, is in practice you have to try <em>all combinations</em> of your hyperparameter values together. This algorithm is often called <strong>Grid Search</strong> because it has to try out a grid of all pairs of hyperparameters. For example, suppose we wanted to try the following values for <span class="math notranslate nohighlight">\(\eta\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta_t \in [0.001, 0.01, 0.1, 1, \frac{1}{t}, \frac{10}{t}]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda \in [0.01, 0.1, 1, 10, 100]\)</span></p></li>
</ul>
<p>Then in total we would have to try 30 combinations of these hyperparamters (<span class="math notranslate nohighlight">\(5 \cdot 6 = 30\)</span>). That is a lot of models to try out and train even for just 5 choices of <span class="math notranslate nohighlight">\(\lambda\)</span> and 6 for <span class="math notranslate nohighlight">\(\eta_t\)</span>.</p>
<!-- TODO add good example for GridSearchCV? -->
<p>It is often tedious to write out code to try all of the options, so scikit-learn provides a <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a> class that does cross validation on all possible settings of hyperparameters you might care to optimize.</p>
</section>
</section>
<section id="logistic-regression-and-overfitting">
<h2><span class="section-number">6.6. </span>Logistic Regression and Overfitting<a class="headerlink" href="#logistic-regression-and-overfitting" title="Permalink to this heading">#</a></h2>
<p>In the concluding sections of this chapter, we want to circle back to our discussion of overfitting and Logistic Regression. We will see a lot of similarities in how overfitting works in our classification setting with Logistic Regression as we did in our module on regression. We won‚Äôt spend much of our time re-hashing the details of overfitting we already know, and will focus more on the effects of overfitting specifically in this new setting.</p>
<!-- TODO cit figure once we have them in the text -->
<p>Logistic Regression is still a linear model, by default, because the decision boundary it learns is still linear. However, just like this didn‚Äôt stop us making Linear Regression more complex, we can do the same with Logistic Regression. One way to learn a more complicated decision boundary, is to use more complex features such as a bi-gram encoding of our text, using polynomial expansions of our features, or any other transformations you can think of to add more complicated features. Generally adding more features and more complex features will mean our resulting model is more complicated.</p>
<!-- TODO I think writing this as a code example with output would be good. Right now it looks kinda ugly too-->
<p>Suppose we had a simplified example of our positive/negative sentiment data, and we just trained it on the counts directly in a simple Logistic Regression Model. If we did that, we would find reasonably sized coefficients. On the right, the graph above shows the raw scores (not applied through the sigmoid) and the picture below shows the resulting decision boundary.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-column sd-col-12 sd-col-xs-12 sd-col-sm-12 sd-col-md-12 sd-col-lg-12 docutils">
<p><strong>Coefficients, Score Function, and Decision Boundary for model with linear features</strong></p>
</div>
<div class="sd-col sd-d-flex-column docutils">
<table class="table" id="logistic-coeffs-1">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_0(x)\)</span></p></td>
<td><p>1</p></td>
<td><p>0.23</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_1(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[1]\)</span></p></td>
<td><p>1.12</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_2(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]\)</span></p></td>
<td><p>-1.07</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-center">
<img alt="A logistic regression decision boundary with linear features" src="../../_images/boundary1.png" />
</figure>
</div>
</div>
</div>
<p>In the figures below, we will show the same figures with increasingly complex feature representations.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-column sd-col-12 sd-col-xs-12 sd-col-sm-12 sd-col-md-12 sd-col-lg-12 docutils">
<p><strong>Coefficients, Score Function, and Decision Boundary for model with quadratic features</strong></p>
</div>
<div class="sd-col sd-d-flex-column docutils">
<table class="table" id="logistic-coeffs-2">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_0(x)\)</span></p></td>
<td><p>1</p></td>
<td><p>1.68</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_1(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[1]\)</span></p></td>
<td><p>1.39</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_2(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]\)</span></p></td>
<td><p>-0.59</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_3(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^2\)</span></p></td>
<td><p>-0.17</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_4(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^2\)</span></p></td>
<td><p>-0.96</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-center">
<img alt="A logistic regression decision boundary with quadratic features" src="../../_images/boundary2.png" />
</figure>
</div>
</div>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-column sd-col-12 sd-col-xs-12 sd-col-sm-12 sd-col-md-12 sd-col-lg-12 docutils">
<p><strong>Coefficients, Score Function, and Decision Boundary for model with degree 6 features</strong></p>
</div>
<div class="sd-col sd-d-flex-column docutils">
<table class="table" id="logistic-coeffs-3">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_0(x)\)</span></p></td>
<td><p>1</p></td>
<td><p>21.6</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_1(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[1]\)</span></p></td>
<td><p>5.3</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_2(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]\)</span></p></td>
<td><p>-42.7</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_3(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^2\)</span></p></td>
<td><p>-15.9</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_4(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^2\)</span></p></td>
<td><p>-48.6</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_5(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^3\)</span></p></td>
<td><p>-11.0</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_6(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^3\)</span></p></td>
<td><p>67.0</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_7(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^4\)</span></p></td>
<td><p>1.5</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_8(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^4\)</span></p></td>
<td><p>48.0</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_9(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^5\)</span></p></td>
<td><p>4.4</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{10}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^5\)</span></p></td>
<td><p>-14.2</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_{11}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^6\)</span></p></td>
<td><p>0.8</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{12}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^6\)</span></p></td>
<td><p>-8.6</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-center">
<img alt="A logistic regression decision boundary with degree 6 features" src="../../_images/boundary3.png" />
</figure>
</div>
</div>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-column sd-col-12 sd-col-xs-12 sd-col-sm-12 sd-col-md-12 sd-col-lg-12 docutils">
<p><strong>Coefficients, Score Function, and Decision Boundary for model with degree 20 features</strong></p>
</div>
<div class="sd-col sd-d-flex-column docutils">
<table class="table" id="logistic-coeffs-4">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_0(x)\)</span></p></td>
<td><p>1</p></td>
<td><p>8.7</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_1(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[1]\)</span></p></td>
<td><p>5.1</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_2(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]\)</span></p></td>
<td><p>78.7</p></td>
</tr>
<tr class="row-odd"><td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{11}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^6\)</span></p></td>
<td><p>-7.5</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_{12}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^6\)</span></p></td>
<td><p><span style="color: red">3803</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{13}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^7\)</span></p></td>
<td><p>21.1</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_{14}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^7\)</span></p></td>
<td><p><span style="color: red">-2406</span></p></td>
</tr>
<tr class="row-even"><td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_{37}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^{19}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2\cdot10^{-6}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{38}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^{19}\)</span></p></td>
<td><p>-0.15</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h_{39}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^{20}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2\cdot10^{-8}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(h_{40}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x[2]^{20}\)</span></p></td>
<td><p>0.03</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/boundary4.png"><img alt="A logistic regression decision boundary with degree 20 features" src="../../_images/boundary4.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
</div>
<p>So just with regression, we can see all of the following properties are shared with classification models as well.</p>
<ul class="simple">
<li><p>Models that have more features / more complicated features tend to be more more complicated. For classification, this means that the decision boundaries look more complex.</p></li>
<li><p>The coefficients in more complicated models tend to have larger magnitudes.</p></li>
<li><p>(Not seen in images above) More complicated models tend to overfit to their training data and generalize poorly.</p></li>
</ul>
<p>In fact, we see the same relationship with classification in terms of its types of errors that we saw with regression in the plot of our error curves below based on model complexity.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/error_curves.png"><img alt="The same training vs true error curves" src="../../_images/error_curves.png" style="width: 100%;" /></a>
</figure>
<p>Now all of this is essentially stuff we have seen before, but there is one other important artifact of overfitting in classification that should be understood: <strong>overconfident predictions</strong>. Recall that our logistic regression model can give us a sense of how confident its predictions are based on the probability outputs. A probability closer to 0 or 1 indicate a sense of certainty that the example should have the label it does. Consider the 4 models we showed above, but showing the probability it predicts for the labels displayed as a color hue around the decision space.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/prob_predictions.png"><img alt="Decision boundaries for the last 4 pictures, but showing probability predictions instead. Big take-away is there are very few moderately-confident predictions in the overfit models." src="../../_images/prob_predictions.png" style="width: 100%;" /></a>
</figure>
<p>In each of these images, the region of uncertainty near the decision boundary where the probability of being either label is near 0.5 shrinks as the model becomes more overfit. It is nice for our models to be certain when they can be, but when something is near the decision boundary, it is healthy to have a little bit of skepticism that your prediction is certainly correct when it‚Äôs possible your decision boundary just happened to be on the wrong side of the decision due to noise. So in our simpler models, we see a healthy region of uncertainty around the decision boundary that shrinks as we increase the polynomial degree of the features. So in classification, seeing most predictions having extreme probabilities is yet another symptom of overfitting.</p>
<p>We will note that this symptom is actually a direct result of the coefficients getting larger in magnitude. With larger magnitudes, we tend to have scores that are also large in magnitude. When applied through our sigmoid function, those large magnitude scores get output as probabilities near 0 or near 1.</p>
</section>
<section id="logistic-regression-and-regularization">
<h2><span class="section-number">6.7. </span>Logistic Regression and Regularization<a class="headerlink" href="#logistic-regression-and-regularization" title="Permalink to this heading">#</a></h2>
<p>While overfitting is completely possible in classification models too, we have a lot of tools under our toolbelt to control for it. If we want to control for overfitting, we can use simpler models, generally done with:</p>
<ul class="simple">
<li><p>Using simpler features in the first place</p></li>
<li><p>Regularization</p></li>
</ul>
<p>We can regularize the Logistic Regression model with a regularizer such as the L1 norm <span class="math notranslate nohighlight">\(\norm{w}_1\)</span> or the L2 norm <span class="math notranslate nohighlight">\(\norm{w}_2^2\)</span>! The great thing is that all of the properties we understand about L1 Regularization (LASSO) and L2 Regularization (Ridge) in the regression setting apply here for Logistic Regression. Refer back to <a class="reference internal" href="../../regression/ridge/index.html"><span class="doc"> Ridge Regularization</span></a> and <a class="reference internal" href="../../regression/lasso/index.html"><span class="doc"> Feature Selection and LASSO Regularization</span></a> for more details on the effects of regularization on our learned models including how choosing the regularization penalty <span class="math notranslate nohighlight">\(\lambda\)</span> affects the model and how to choose the right setting of <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>The one minor details that differs that we should note is the exact formulation of the regularized quality metric. Note that in our setting where we are now <em>maximizing</em> a function, we will want to <em>subtract</em> our regularizer term to make the overfit models look worse when compared by the regularized quality metric. For example, using Logistic Regression with the L2 norm regularizer would optimize the following regularized version of the likelihood:</p>
<div class="math notranslate nohighlight">
\[\hat{w} = \argmax{w}\log(\ell(w)) - \lambda \norm{w}_2^2\]</div>
</section>
<section id="recap">
<h2><span class="section-number">6.8. </span>Recap<a class="headerlink" href="#recap" title="Permalink to this heading">#</a></h2>
<p>In this chapter, we introduced our first complete classification model with Logistic Regression. Along the way, we introduced some important ideas to go along with the concept.</p>
<ul class="simple">
<li><p>Predicting with probabilities</p></li>
<li><p>Using the logistic/sigmoid function to convert scores to probabilities</p></li>
<li><p>Logistic Regression</p></li>
<li><p>Minimizing error vs. maximizing likelihood</p></li>
<li><p>Gradient Ascent</p></li>
<li><p>Effects of changing <span class="math notranslate nohighlight">\(\eta\)</span>, step size, learning rate</p></li>
<li><p>Overfitting with logistic regression</p></li>
<li><p>Regularization with logistic regression</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./classification/logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span><i class="fas fa-book fa-fw"></i> Classification Overview</p>
      </div>
    </a>
    <a class="right-next"
       href="../bias_fairness/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span><i class="fas fa-book fa-fw"></i> Bias and Fairness</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-classification-error">6.1. Minimizing Classification Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-predictions">6.2. Probability Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">6.3. Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scores-to-probabilities-a-sigmoid-approach">6.3.1. Scores to Probabilities: A Sigmoid Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-model">6.3.2. Logistic Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-code">6.3.3. Logistic Regression Code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">6.4. Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-ascent">6.5. Gradient Ascent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-size">6.5.1. Step Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">6.5.2. Grid Search</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-and-overfitting">6.6. Logistic Regression and Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-and-regularization">6.7. Logistic Regression and Regularization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">6.8. Recap</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hunter Schafer
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div style="float: right">
  <!-- 100% privacy friendly analytics -->
  <script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
  <noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>
  <a href="https://simpleanalytics.com/?utm_source=&utm_content=badge" referrerpolicy="origin" target="_blank"><picture><source srcset="https://simpleanalyticsbadges.com/?mode=dark" media="(prefers-color-scheme: dark)" /><img src="https://simpleanalyticsbadges.com/?mode=light" loading="lazy" referrerpolicy="no-referrer" crossorigin="anonymous" /></picture></a>
</div>

<div>
  <p>
    Have feedback or spotted a bug? Please make a <a href="https://github.com/animlbook/AnIML/issues">GitHub issue</a>
    or contact <a href="https://homes.cs.washington.edu/~hschafer/">Hunter Schafer</a>!
  </p>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>